{"cells":[{"cell_type":"markdown","metadata":{"id":"9hZNSDOBJcBm"},"source":["# Assignment II: Pretraining & Fine-Tuning of Language Models\n","\n","In this second assignment we will continue to work with PyTorch and Open AI's early Open Source Model GPT2 to develop a deeper understanding and intuition of how language models are trained. We will look at a specific simple task, Sentiment Classification, and see in two ways how we can use language models for this problem.\n","\n","The structure of the Assignment is as follows:\n","\n","1. **Continued GPT-2 Pretraining of GPT with a Movie Plots dataset**  \n","\n","   Here we will explore how continued pretraining affects a Language Model. This gives us a good idea how pretraining morks, and more specifically, we will look how additional domain-specific pretraining affects the perplexity for text within this domain vs outside of the domain. We will use the *CMU Movie Summary Corpus* (https://www.cs.cmu.edu/~ark/personas/), released under the Creative Commons Attribution-ShareAlike License (http://creativecommons.org/licenses/by-sa/3.0/us/legalcode).\n","   We will learn that additional pretraining generally helps language models for in-domain tasks.\n","\n","3. **Fine-tuning of GPT-2 for Sentiment Analysis of the IMDB dataset (using Pre/Post-Modifiers)**  \n","   We will then use both the original GPT-2 model and the model that was further pretrained on the CMU Movie Summary dataset for a Sentiment Analysis of the IMDB movie review dataset, which is part of Hugging Face datasets. We will (hopefully(!)... there are statistical fluctuations) see that fine-tuning the model that was further pre-trained on the movie plot dataset behaves somewhat better than the original gpt-2 model fine-tuned.\n","\n","4. **IMDB Sentiment Classification with a Masked Language Model (BERT)**\n","   Finally, we will also look at a Masked Language Model, specifically BERT, as a tool for Sentiment Classification of the same dataset.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"L0IwczMZFtrF"},"source":["For reference, please consider the Lecture material for weeks 2 & 3 as well as the two Special Session notebooks:\n","\n","* Intro to PyTorch I (Basics)\n","* Intro to PyTorch II (Huggingface & Language Models)\n","* All lesson material and notebooks to date\n","\n","\n","\n","**INSTRUCTIONS:**\n","\n","* This notebook needs to be run using a GPU. If you use Google Colab, a T4 chip is the recommendation.\n","  \n","* Questions are always indicated as **QUESTION:**, so you can search for this string to make sure you answered all of the questions. You are expected to fill out, run, and submit this notebook, as well as to answer the questions in the answers file as you did in a1. Please do not remove the output from your notebooks when you submit them as we'll look at the output as well as your code for grading purposes.\n","\n","* \\### YOUR CODE HERE indicates that you are supposed to write code. All the way up to \\### END YOUR CODE     \n","\n","* **Important!!:** When you are done please re-run your notebook from beginning to end to that all of the seeds apply! This is very important!\n","\n"]},{"cell_type":"markdown","metadata":{"id":"peRi8BBqFtrF"},"source":["## 0. Setup\n","\n","Let us first install a few required packages. (You may want to comment this out in case you use a local environment that already has the suitable packages installed.)"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"GuN074G-JK_T"},"outputs":[],"source":["%%capture\n","\n","#!pip install torch           # not required for colabs. Uncomment if needed otherwise\n","#!pip install transformers    # not required for colabs. Uncomment if needed otherwise\n","#!pip install numpy           # not required for colabs. Uncomment if needed otherwise\n","!pip install portalocker\n","!pip install datasets         # Hugging Face's dataset library\n","#!pip install pandas          # not required for colabs. Uncomment if needed otherwise"]},{"cell_type":"markdown","metadata":{"id":"ymhMnEdJFtrF"},"source":["Next, we will import required libraries"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"NySGJMIlFtrF"},"outputs":[],"source":["import copy\n","import random\n","\n","import torch\n","import numpy as np\n","import pandas as pd\n","\n","from datasets import load_dataset\n","\n","#from torchtext import data as torchtext_data\n","from torch import nn\n","\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import AutoTokenizer, GPT2Model, GPT2ForSequenceClassification, GPT2LMHeadModel"]},{"cell_type":"markdown","metadata":{"id":"LZmkkCXxuf0o"},"source":["Let's make sure we will later put data and models on the proper device."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"9piY_5XnuVa9"},"outputs":[{"data":{"text/plain":["device(type='cpu')"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","#device = torch.device(\"mps\")               # in case you run on a local Mac with metal performance shaders (setup/support is up to you)\n","device"]},{"cell_type":"markdown","metadata":{"id":"t91i97P4FtrG"},"source":["Now we will define various functions that we will use in this nodebook. You can jump over this part... but you don't have to.\n","\n","We'll define:\n","\n","1. perplexity(text, model) - a calculation giving us the perplexity for a given text and model. 'How certain is the model in picking the actual nect token?'\n","2. ClassificationData class - a class that created the Dataset for our IMDB classification with GPT-2. It has a number of options that we'll use to augment the text to make the classification easier for the model.\n","3. BERT ClassificationData class - same for a BERT model.\n","4. create_temp_set(base_data, split) - a function used to massage the IMDB dataset, just as we did in PyTorch Intro I.\n","5. random_huggingface_blog_text - A list of text of random Hugging Face blog snippets for some validation tests.\n","6. fake_data_loader -  a function that converts an array of text (fixed batch size for now) into a format that the perplexity calculation can use.\n"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"vbplEALAFtrG"},"outputs":[],"source":["#@title Some Definitions\n","\n","def perplexity(text_data, model):\n","\n","    loss = []\n","    for step, batch_data in enumerate(text_data):\n","\n","      batch_input = batch_data['input']\n","      batch_labels = batch_data['labels']\n","\n","      if step % 100 == 0:\n","          print('Current batch: ', step)\n","\n","      with torch.no_grad():\n","            try:\n","              batch_output = model(batch_input)\n","              batch_output_reshaped = batch_output.reshape((batch_size * (max_len - 1), -1))\n","\n","              batch_labels_reshaped = batch_labels.reshape((batch_size * (max_len - 1),))\n","              batch_loss = loss_fn(batch_output_reshaped, batch_labels_reshaped)\n","\n","              loss.append(batch_loss)\n","            except:\n","              continue\n","\n","    avg_cost = np.mean([x.cpu().detach() for x in loss])\n","    avg_perplexity = np.exp(avg_cost)\n","\n","    return avg_perplexity\n","\n","\n","class ClassificationData(Dataset):\n","    def __init__(self,\n","                 base_data,\n","                 tokenizer,\n","                 max_len,\n","                 use_prompt=False,\n","                 prompt_pre_text='',\n","                 prompt_post_text='',\n","                 classification_tokenset={1: 'good', 0: 'bad'},\n","                 num_examples=-1):\n","\n","        self.max_len = max_len\n","        self.tokenizer = tokenizer  # assume that padding token has already been added to tokenizer\n","        self.data = []\n","\n","        # really  not ideal having to iterate through the whole set. But ok for this small data volume\n","\n","\n","        prompt_pre_text = prompt_pre_text.strip()\n","        prompt_post_text = prompt_post_text.strip()\n","\n","\n","        for num_example, example in enumerate(base_data):\n","\n","            if num_examples != -1 and num_example >= num_examples:\n","              break\n","\n","            if num_example == 0:\n","              print(example)\n","\n","            stripped_example = example['text'].strip()\n","\n","            token_encoder = self.tokenizer(' ' + stripped_example)['input_ids'] # simulating that the text will not be at the beginning\n","\n","            if len(token_encoder) <= self.max_len:\n","                continue    # avoids complications with short sentences. No padding is needed then.\n","\n","            truncated_encoding = token_encoder[:self.max_len]\n","            truncated_example = tokenizer.decode(truncated_encoding) # reconstruct shortened review\n","\n","\n","            # LLMs do next-word predictions. You may want to add a prompt that the model can work with!\n","\n","\n","            if use_prompt:\n","\n","                additional_token_length = len(self.tokenizer(prompt_pre_text)['input_ids']) + len(self.tokenizer(' ' + prompt_post_text)['input_ids'])  # simulating that the prompt_post_text will not be at the beginning\n","\n","                cutoff = self.max_len + additional_token_length\n","\n","                prompted_text_line = prompt_pre_text + truncated_example + ' ' + prompt_post_text\n","\n","            else:\n","                cutoff = self.max_len\n","                prompted_text_line = truncated_example\n","\n","            if len(self.tokenizer(prompted_text_line)['input_ids']) != cutoff:\n","                    continue\n","\n","\n","\n","            tokenized_example = self.tokenizer(prompted_text_line,\n","                                               return_tensors=\"pt\",\n","                                               max_length=cutoff,\n","                                               truncation=True,\n","                                               padding='max_length').to(device)\n","\n","            if example['label'] == 1:\n","              token = classification_tokenset[1]\n","            else:\n","              token = classification_tokenset[0]\n","\n","            token_id = self.tokenizer.encode(' ' + token)[0]\n","            label = torch.tensor(token_id, dtype=torch.int64, device=device)\n","\n","            self.data.append({'label': label,\n","                              'input_ids': torch.squeeze(tokenized_example['input_ids']).to(device)\n","                              })\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","\n","        return {\n","            'input_ids': self.data[index]['input_ids'],\n","            'label': self.data[index]['label']\n","        }\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","\n","        return {\n","            'input_ids': self.data[index]['input_ids'],\n","            'label': self.data[index]['label']\n","        }\n","\n","\n","class BERTClassificationData(Dataset):\n","    def __init__(self,\n","                 base_data,\n","                 tokenizer,\n","                 max_len,\n","                 num_examples=-1):\n","\n","        self.max_len = max_len\n","        self.tokenizer = tokenizer  # assume that padding token has already been added to tokenizer\n","        self.data = []\n","\n","        # really  not ideal having to iterate through the whole set. But ok for this small data volume\n","\n","\n","\n","        for num_example, example in enumerate(base_data):\n","\n","            if num_examples != -1 and num_example >= num_examples:\n","              break\n","\n","\n","            token_encoder = self.tokenizer(example['text'])['input_ids']\n","\n","            if len(token_encoder) <= self.max_len:\n","                continue    # avoids complications with short sentences. No padding is needed then.\n","\n","            truncated_encoding = token_encoder[1:self.max_len + 1]\n","            truncated_example = tokenizer.decode(truncated_encoding) # reconstruct shortened review\n","\n","            cutoff = self.max_len\n","            prompted_text_line = truncated_example\n","\n","            tokenized_example = self.tokenizer(prompted_text_line,\n","                                               return_tensors=\"pt\",\n","                                               max_length=cutoff,\n","                                               truncation=True,\n","                                               padding='max_length').to(device)\n","\n","            label_val = example['label']\n","\n","            label = torch.tensor(label_val, dtype=torch.int64, device=device)\n","\n","            self.data.append({'label': label,\n","                              'input_ids': torch.squeeze(tokenized_example['input_ids']).to(device)\n","                              })\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","\n","        return {\n","            'input_ids': self.data[index]['input_ids'],\n","            'label': self.data[index]['label']\n","        }\n","\n","\n","def create_temp_set(base_data, num_examples=1000000000):\n","    num_positive = 0\n","    num_negative = 0\n","    num_other = 0\n","\n","    temp_data = []\n","    out_data = []\n","\n","    for example_num, example in enumerate(base_data):\n","\n","      temp_data.append(example)\n","\n","    random.shuffle(temp_data)\n","\n","    for example_num, example in enumerate(temp_data):\n","\n","      if num_examples != -1 and example_num > num_examples:\n","        break\n","\n","      if example['label'] == 0:\n","        num_negative += 1\n","      elif example['label'] == 1:\n","        num_positive += 1\n","      else:\n","        num_other += 1\n","\n","      out_data.append(example)\n","\n","\n","    print('positive: ', num_positive)\n","    print('negative: ', num_negative)\n","    print('other: ', num_other)\n","\n","    return out_data\n","\n","\n","random_huggingface_blog_text = [\"\"\"1. Aspect candidate extraction\n","\n","In this work we assume that aspects, which are usually features of products and services, are mostly nouns or noun compounds (strings of consecutive nouns). We use spaCy to tokenize and extract nouns/noun compounds from the sentences in the (few-shot) training set. Since not all extracted nouns/noun compounds are aspects, we refer to them as aspect candidates.\n","\n","2. Aspect/Non-aspect classification\n","\n","Now that we have aspect candidates, we need to train a model to be able to distinguish between nouns that are aspects and nouns that are non-aspects. For this purpose, we need training samples with aspect/no-aspect labels. This is done by considering aspects in the training set as True aspects, while other non-overlapping candidate aspects are considered non-aspects and therefore labeled as False:\n","\n","Training sentence: \"Waiters aren't friendly but the cream pasta is out of this world.\"\n","Tokenized: [Waiters, are, n't, friendly, but, the, cream, pasta, is, out, of, this, world, .]\n","Extracted aspect candidates: [Waiters, are, n't, friendly, but, the, cream, pasta, is, out, of, this, world, .]\n","Gold labels from training set, in BIO format: [B-ASP, O, O, O, O, O, B-ASP, I-ASP, O, O, O, O, O, .]\n","Generated aspect/non-aspect Labels: [Waiters, are, n't, friendly, but, the, cream, pasta, is, out, of, this, world, .]\n","Now that we have all the aspect candidates labeled, how do we use it to train the candidate aspect classification model? In other words, how do we use SetFit, a sentence classification framework, to classify individual tokens? Well, this is the trick: each aspect candidate is concatenated with the entire training sentence to create a training instance using the following template:\"\"\",\n","             \"\"\"Normalization interrogations\n","During our first deeper dive in these surprising behavior, we observed that the normalization step was possibly not working as intended: in some cases, this normalization ignored the correct numerical answers when they were directly followed by a whitespace character other than a space (a line return, for example). Let's look at an example, with the generation being 10\\n\\nPassage: The 2011 census recorded a population of 1,001,360, and the gold answer being 10.\n","\n","Normalization happens in several steps, both for generation and gold:\n","\n","Split on separators |, -, or The beginning sequence of the generation 10\\n\\nPassage: contain no such separator, and is therefore considered a single entity after this step.\n","Punctuation removal The first token then becomes 10\\n\\nPassage (: is removed)\n","Homogenization of numbers Every string that can be cast to float is considered a number and cast to float, then re-converted to string. 10\\n\\nPassage stays the same, as it cannot be cast to float, whereas the gold 10 becomes 10.0.\n","Other steps A lot of other normalization steps ensue (removing articles, removing other whitespaces, etc.) and our original example becomes 10 passage 2011.0 census recorded population of 1001360.0.\n","However, the overall score is not computed on the string, but on the bag of words (BOW) extracted from the string, here {'recorded', 'population', 'passage', 'census', '2011.0', '1001360.0', '10'}, which is compared with the BOW of the gold, also normalized in the above manner, {10.0}. As you can see, they don’t intersect, even though the model predicted the correct output!\n","\n","In summary, if a number is followed by any kind of whitespace other than a simple space, it will not pass through the number normalization, hence never match the gold if it is also a number! This first issue was likely to mess up the scores quite a bit, but clearly it was not the only factor causing DROP scores to be so low. We decided to investigate a bit more.\n","\n","Diving into the results\n","Extending our investigations, our friends at Zeno joined us and undertook a much more thorough exploration of the results, looking at 5 models which were representative of the problems we noticed in DROP scores: falcon-180B and mistral-7B were underperforming compared to what we were expecting, Yi-34B and tigerbot-70B had a very good performance on DROP correlated with their average scores, and facebook/xglm-7.5B fell in the middle.\n","\n","You can give analyzing the results a try in the Zeno project here if you want to!\n","\n","The Zeno team found two even more concerning features:\n","\n","Not a single model got a correct result on floating point answers\n","High quality models which generate long answers actually have a lower f1-score\n","At this point, we believed that both failure cases were actually caused by the same root factor: using . as a stopword token (to end the generations):\n","\n","Floating point answers are systematically interrupted before their generation is complete\n","Higher quality models, which try to match the few-shot prompt format, will generate Answer\\n\\nPlausible prompt for the next question., and only stop during the plausible prompt continuation after the actual answer on the first ., therefore generating too many words and getting a bad f1 score.\n","We hypothesized that both these problems could be fixed by using \\n instead of . as an end of generation stop word.\"\"\",\n","             \"\"\"Text generation is a rich topic, and there exist several generation strategies for different purposes. We recommend this excellent overview on the subject. Many generation algorithms are supported by the text generation endpoints, and they can be configured using the following parameters:\n","\n","do_sample: If set to False (the default), the generation method will be greedy search, which selects the most probable continuation sequence after the prompt you provide. Greedy search is deterministic, so the same results will always be returned from the same input. When do_sample is True, tokens will be sampled from a probability distribution and will therefore vary across invocations.\n","temperature: Controls the amount of variation we desire from the generation. A temperature of 0 is equivalent to greedy search. If we set a value for temperature, then do_sample will automatically be enabled. The same thing happens for top_k and top_p. When doing code-related tasks, we want less variability and hence recommend a low temperature. For other tasks, such as open-ended text generation, we recommend a higher one.\"\"\",\n","             \"\"\"Recently, we released our Object Detection Leaderboard, ranking object detection models available in the Hub according to some metrics. In this blog, we will demonstrate how the models were evaluated and demystify the popular metrics used in Object Detection, from Intersection over Union (IoU) to Average Precision (AP) and Average Recall (AR). More importantly, we will spotlight the inherent divergences and pitfalls that can occur during evaluation, ensuring that you're equipped with the knowledge not just to understand but to assess model performance critically.\n","\n","Every developer and researcher aims for a model that can accurately detect and delineate objects. Our Object Detection Leaderboard is the right place to find an open-source model that best fits their application needs. But what does \"accurate\" truly mean in this context? Which metrics should one trust? How are they computed? And, perhaps more crucially, why some models may present divergent results in different reports? All these questions will be answered in this blog.\n","\n","So, let's embark on this exploration together and unlock the secrets of the Object Detection Leaderboard! If you prefer to skip the introduction and learn how object detection metrics are computed, go to the Metrics section. If you wish to find how to pick the best models based on the Object Detection Leaderboard, you may check the Object Detection Leaderboard section.\"\"\"]\n","\n","def fake_data_loader(text, tokenizer, max_len):\n","  return [{'input': tokenizer(text, return_tensors='pt', truncation=True, max_length=max_len)['input_ids'][:, :max_len-1].to(device),\n","            'labels': tokenizer(text, return_tensors='pt', truncation=True, max_length=max_len)['input_ids'][:, 1:max_len].to(device)}]\n"]},{"cell_type":"markdown","metadata":{"id":"QEh7So11FtrG"},"source":["This should say 'cpu' if using a CPU, or 'cuda', if a GPU is used (or 'mps' per the comment)."]},{"cell_type":"markdown","metadata":{"id":"CiXfjjjGFtrG"},"source":["Now we are ready to move to Language Models.\n","\n","## 1. Continued Pretraining of GPT2 with a Movie Plots dataset\n","\n","We are now downloading GPT-2 from Hugging Face, i.e. the tokenizer and the model. We will i) make sure that it is on the proper device, and ii) copy the model to a second one that will see additional pre-training before being used for a classification task."]},{"cell_type":"code","execution_count":5,"metadata":{"id":"rTut1Px2FtrG"},"outputs":[],"source":["%%capture\n","\n","torch.manual_seed(10)\n","random.seed(10)\n","np.random.seed(10)\n","\n","gpt_2_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n","gpt_2_tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","\n","base_gpt2_model = GPT2LMHeadModel.from_pretrained(\"gpt2\").to(device)\n","additinal_pretrain_gpt2_model = copy.deepcopy(base_gpt2_model)\n"]},{"cell_type":"markdown","metadata":{"id":"Gav_H2C8FtrH"},"source":["We will now continue to pretrain the model *additinal_pretrain_gpt2_model* on a dataset of a specific domain - movies. We use the *CMU Movie Summary Corpus* (https://www.cs.cmu.edu/~ark/personas/, license: http://creativecommons.org/licenses/by-sa/3.0/us/legalcode). It contains 40k+ unlabeled movie plot summaries. As such, they represent domain-specific text which is available at many companies and institutions using their internal documents.\n","\n","Get the dataset by uncommenting the first line below (we have it commented here because you may need to rerun the notebook multiple times when you already have the dataset. When you do that... make sure you comment out this line again):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y6V4R2VDFZ3h"},"outputs":[],"source":["#!wget https://www.cs.cmu.edu/~ark/personas/data/MovieSummaries.tar.gz\n","# We reccomend downloading the file! You can then upload the file later when needed from your local computer. Go to the folder icon on the left."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R3VRuFsevZb8"},"outputs":[],"source":["#!tar -xvf MovieSummaries.tar.gz"]},{"cell_type":"markdown","metadata":{"id":"-YSDM6Z9FtrH"},"source":["Next, we will create a list of 15k plots and convince ourselves that the data looks roughly as expected:"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"e9Z8Ivc0FqiH"},"outputs":[{"data":{"text/plain":["'The nation of Panem consists of a wealthy Capitol and twelve poorer districts. As punishment for a past rebellion, each district must provide a boy and girl  between the ages of 12 and 18 selected by lottery  for the annual Hunger Games. The tributes must fight to the death in an arena; the sole survivor is rewarded with fame and wealth. In her first Reaping, 12-year-old Primrose Everdeen is chose'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["plots =  pd.read_csv('MovieSummaries/plot_summaries.txt', delimiter='\\t')\n","plots.columns = ['id', 'plot']\n","plot_summary_list = [x for x in plots['plot']][:15000]\n","plot_summary_list[0][:400]"]},{"cell_type":"markdown","metadata":{"id":"SAR7ej4nlHdb"},"source":["Next, we will create a Dataset class that takes text data and returns input token ids and labels for the next word predictions (simply the input token ids shifted one to the left). For simplicity, we will throw out any examples that are shorter than our desired length and truncate all other examples to this length:"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"N9xh-xTXRzYD"},"outputs":[],"source":["#@title Class for Creation of Continued Pretraining Dataset (Movie Plots)\n","\n","class ContinuedPretrainData(Dataset):\n","    def __init__(self, base_data, tokenizer, max_len, device):\n","        self.max_len = max_len\n","        self.tokenizer = tokenizer  # assume that padding token has already been added to tokenizer\n","        self.data = []\n","\n","        tokenized_examples = tokenizer(base_data,\n","                                       max_length=max_len,\n","                                       truncation=True, padding='max_length',\n","                                       return_tensors=\"pt\")\n","\n","        tokens = tokenized_examples['input_ids'][tokenized_examples['attention_mask'][:, max_len - 1] > 0]\n","\n","        self.data = tokens.to(device)\n","\n","    def __len__(self):\n","        return self.data.shape[0]\n","\n","    def __getitem__(self, index):\n","\n","        return {'input': self.data[index][:self.max_len - 1],\n","                'labels': self.data[index][1:]\n","        }"]},{"cell_type":"markdown","metadata":{"id":"9nFW8dovRG0D"},"source":["Now, please build a simple neural net that serves for continued pre-training:"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"OKG3AXjSRzpZ"},"outputs":[],"source":["%%capture\n","\n","class ContinuedTrainingNetwork(torch.nn.Module):\n","    \"\"\"\n","    Build a simple PyTorch network that takes a batch (from a Dataloader)\n","    as input and returns the logits of each next word prediction.\n","    When instantiated, you need to pass in a pretrained base model.\n","    You need to define both, the __init__ and the forward methods.\n","    \"\"\"\n","    def __init__(self, pretrainModel):\n","\n","         ### YOUR CODE HERE\n","\n","        super(ContinuedTrainingNetwork, self).__init__()\n","        self.pretrained_model = pretrainModel\n","\n","        ### END YOUR CODE\n","\n","    def forward(self, x):                             # x stands for the input that the network will use/act on later\n","        # get the logits for all tokens in all examples and call it logits.\n","\n","        ### YOUR CODE HERE\n","\n","        outputs = self.pretrained_model(input_ids=input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits  \n","        \n","        ### END YOUR CODE\n","\n","        return logits\n","\n","pretrain_network = ContinuedTrainingNetwork(pretrainModel=additinal_pretrain_gpt2_model)\n","\n","pretrain_network.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UUATy7vp5vDA"},"outputs":[],"source":["#... = {'input': tensor([...])}\n"]},{"cell_type":"markdown","metadata":{"id":"tv4d8UFCFtrH"},"source":["Then we create the training sets:"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"JpoJG71tRzg-"},"outputs":[],"source":["max_len=100\n","\n","train_data = ContinuedPretrainData(plot_summary_list[:10000], tokenizer=gpt_2_tokenizer, max_len=max_len, device=device)\n","test_data = ContinuedPretrainData(plot_summary_list[10000:], tokenizer=gpt_2_tokenizer, max_len=max_len, device=device)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"PUss4GF7Rzjt"},"outputs":[{"data":{"text/plain":["{'input': tensor([  464,  3277,   286,  5961,   368, 10874,   286,   257, 11574, 13241,\n","           290, 14104, 26647, 12815,    13,  1081,  9837,   329,   257,  1613,\n","         21540,    11,  1123,  4783,  1276,  2148,   257,  2933,   290,  2576,\n","           220,  1022,   262,  9337,   286,  1105,   290,  1248,  6163,   416,\n","         22098,   220,   329,   262,  5079, 32367,  5776,    13,   383,   256,\n","          7657,  1276,  1907,   284,   262,  1918,   287,   281, 13478,    26,\n","           262,  6195, 23446,   318, 20945,   351, 16117,   290,  5129,    13,\n","           554,   607,   717,   797,  9269,    11,  1105,    12,  1941,    12,\n","           727, 11460, 13698, 10776, 39060,   318,  7147,   422,  5665,  1105,\n","            13,  2332,  4697,  6621,  8595,    77,   747, 11661,   284]),\n"," 'labels': tensor([ 3277,   286,  5961,   368, 10874,   286,   257, 11574, 13241,   290,\n","         14104, 26647, 12815,    13,  1081,  9837,   329,   257,  1613, 21540,\n","            11,  1123,  4783,  1276,  2148,   257,  2933,   290,  2576,   220,\n","          1022,   262,  9337,   286,  1105,   290,  1248,  6163,   416, 22098,\n","           220,   329,   262,  5079, 32367,  5776,    13,   383,   256,  7657,\n","          1276,  1907,   284,   262,  1918,   287,   281, 13478,    26,   262,\n","          6195, 23446,   318, 20945,   351, 16117,   290,  5129,    13,   554,\n","           607,   717,   797,  9269,    11,  1105,    12,  1941,    12,   727,\n","         11460, 13698, 10776, 39060,   318,  7147,   422,  5665,  1105,    13,\n","          2332,  4697,  6621,  8595,    77,   747, 11661,   284,  1011])}"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["train_data[0]"]},{"cell_type":"markdown","metadata":{"id":"C0Z4ZV6GgTAH"},"source":["Here is the data loader:"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"yEFJx0qiRzmJ"},"outputs":[],"source":["torch.manual_seed(10)\n","batch_size = 4\n","train_texts = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n","test_texts = DataLoader(test_data, batch_size=batch_size, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"s50t7hGkFtrH"},"source":["Next, we construct a network that takes the input from the loaders and returns the logits of each next word prediction:"]},{"cell_type":"markdown","metadata":{"id":"5i-Z0llqFtrH"},"source":["Test the shape of the output. Is it correct? We first need to grab an example and then look at the shape of the model output:"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input': tensor([[38743,   494,    11, 40349,    11,   290,  1338, 39556,   338, 23169,\n","          4502,   389,   287,  3240,    13, 23169,  4502, 15314,   257,   905,\n","          1444,   366, 25946,  1869,   422,   347,  8553,    78,  1600,  9593,\n","           257, 44897,   351,   257, 21332,   286,   257,  3598,    12,  1941,\n","            12,   727,  1200,    13,   383,  3988,     6,  2988, 17567,   284,\n","          1309,   262,  1103, 23169,  4502,  1282,   625,   523,   511,  2802,\n","           468,   262,  3988,  3187,   683,   379,   262,   905,   338,  4067,\n","            13,  5334,  2802,  6688,   284,   262,  3988,   326, 23169,  4502,\n","           318,   262,  2042, 15900,   286,   262,  1641,    13,  1119,  1282,\n","           284,   262,   905,  4067,   290,  1194,  8383,  4952,   262],\n","        [  818,  4842,    11, 37211, 37547,    11, 41492, 34821,   494, 11345,\n","           220, 11864,   257, 26617,    11,   810,  3491, 17378,  2611,  9077,\n","           220, 17706,   764, 34821,   494,   318,   612,   351,  8559,  8913,\n","           306,   837,   257, 18854,   673,  7176,   481,   651,   607,   257,\n","           410,  3885,  1990,  8270, 12526,    13,  2293,   262,   905,    11,\n","         17378,  2611,   318,  5169,   329,  5170,   607,  5229,   290,  6621,\n","           706,  4917,   606,   287,  3996,  1978,    13,   317,   614,  8318,\n","            11,   290,  8559, 10069,   284, 34821,   494,   326,   339, 19837,\n","           546,   465,  8787,   287,  1502,   284,  3993,   351,   607,   290,\n","           450,   392,   684,   607,    11,  9008,   607,   618,   673],\n","        [33056,  9375,   272,  4434,  8940,   430,   220,   468,  6875,   326,\n","           339,   481, 11005,   477,  7706,  5937,   422, 22917,  1626,   734,\n","          1933,    13, 19228,  1706, 28793,   343,   275, 44488,   220, 13267,\n","           284, 11005,  7125,    47,  9375,   272,  2427,    13,  1406,   287,\n","          2058, 49219, 14396,   837,   257,  2277,   805,   508,  5860,   284,\n","         22917,   706,   257,   890, 24329,   287,  6342,    11,  4881,   287,\n","          1502,   284,  1620,   530,   938,  1693,    13,  2893,   262,  7706,\n","          5937,   290, 49219, 14396,  1949,   284,  1011,   866,  7125,    47,\n","          9375,   272,    11,  7125,    47,  9375,   272,   318,  2111,   284,\n","         36440,   465,  1468,  4152,  1545,   309, 34183,   220,   290],\n","        [27007,  3506, 11709,   554, 24648,    11,   262, 49320,   461, 11231,\n","          1641, 15398,  2737,   618,   257,  6403,  6240,   290,   465,   734,\n","         14850,    11,   311, 40063,   290, 42244,   220,  1445,   656,   281,\n","          1468,  2156,   287, 10016,  2869,   284,   307,  5699,   284,   262,\n","          4436,   810,   511,  2802,   318, 20222,   422,   281, 23226,    11,\n","           890,    12,  4354,  8526,    13,   383, 14850,  1064,   326,   262,\n","          2156,   318, 30671,   416,  7009, 15108,  8977,  8109,  1444,  2341,\n","            84,    86, 35554,   851,  1402,    11,  3223,    11,  8977,    12,\n","          2339,  2156, 13747,  1775,   618,  3867,   422,  1657,   284,  3223,\n","          4113,    13,  1649,   262,  4813,  1716,  6792,   287,   511]]), 'labels': tensor([[  494,    11, 40349,    11,   290,  1338, 39556,   338, 23169,  4502,\n","           389,   287,  3240,    13, 23169,  4502, 15314,   257,   905,  1444,\n","           366, 25946,  1869,   422,   347,  8553,    78,  1600,  9593,   257,\n","         44897,   351,   257, 21332,   286,   257,  3598,    12,  1941,    12,\n","           727,  1200,    13,   383,  3988,     6,  2988, 17567,   284,  1309,\n","           262,  1103, 23169,  4502,  1282,   625,   523,   511,  2802,   468,\n","           262,  3988,  3187,   683,   379,   262,   905,   338,  4067,    13,\n","          5334,  2802,  6688,   284,   262,  3988,   326, 23169,  4502,   318,\n","           262,  2042, 15900,   286,   262,  1641,    13,  1119,  1282,   284,\n","           262,   905,  4067,   290,  1194,  8383,  4952,   262,  3988],\n","        [ 4842,    11, 37211, 37547,    11, 41492, 34821,   494, 11345,   220,\n","         11864,   257, 26617,    11,   810,  3491, 17378,  2611,  9077,   220,\n","         17706,   764, 34821,   494,   318,   612,   351,  8559,  8913,   306,\n","           837,   257, 18854,   673,  7176,   481,   651,   607,   257,   410,\n","          3885,  1990,  8270, 12526,    13,  2293,   262,   905,    11, 17378,\n","          2611,   318,  5169,   329,  5170,   607,  5229,   290,  6621,   706,\n","          4917,   606,   287,  3996,  1978,    13,   317,   614,  8318,    11,\n","           290,  8559, 10069,   284, 34821,   494,   326,   339, 19837,   546,\n","           465,  8787,   287,  1502,   284,  3993,   351,   607,   290,   450,\n","           392,   684,   607,    11,  9008,   607,   618,   673,  8536],\n","        [ 9375,   272,  4434,  8940,   430,   220,   468,  6875,   326,   339,\n","           481, 11005,   477,  7706,  5937,   422, 22917,  1626,   734,  1933,\n","            13, 19228,  1706, 28793,   343,   275, 44488,   220, 13267,   284,\n","         11005,  7125,    47,  9375,   272,  2427,    13,  1406,   287,  2058,\n","         49219, 14396,   837,   257,  2277,   805,   508,  5860,   284, 22917,\n","           706,   257,   890, 24329,   287,  6342,    11,  4881,   287,  1502,\n","           284,  1620,   530,   938,  1693,    13,  2893,   262,  7706,  5937,\n","           290, 49219, 14396,  1949,   284,  1011,   866,  7125,    47,  9375,\n","           272,    11,  7125,    47,  9375,   272,   318,  2111,   284, 36440,\n","           465,  1468,  4152,  1545,   309, 34183,   220,   290,   309],\n","        [ 3506, 11709,   554, 24648,    11,   262, 49320,   461, 11231,  1641,\n","         15398,  2737,   618,   257,  6403,  6240,   290,   465,   734, 14850,\n","            11,   311, 40063,   290, 42244,   220,  1445,   656,   281,  1468,\n","          2156,   287, 10016,  2869,   284,   307,  5699,   284,   262,  4436,\n","           810,   511,  2802,   318, 20222,   422,   281, 23226,    11,   890,\n","            12,  4354,  8526,    13,   383, 14850,  1064,   326,   262,  2156,\n","           318, 30671,   416,  7009, 15108,  8977,  8109,  1444,  2341,    84,\n","            86, 35554,   851,  1402,    11,  3223,    11,  8977,    12,  2339,\n","          2156, 13747,  1775,   618,  3867,   422,  1657,   284,  3223,  4113,\n","            13,  1649,   262,  4813,  1716,  6792,   287,   511,   649]])}\n"]}],"source":["example_data = next(iter(test_texts))\n","print(example_data)  # This will show you what keys are available"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"V6yUS2tQRztk"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([4, 99, 50257])\n"]},{"data":{"text/plain":["torch.Size([4, 99, 50257])"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["example_data = next(iter(test_texts))\n","\n","# Please call your model output pretrain_model_output\n","\n","### YOUR CODE HERE\n","\n","input_ids = example_data['input'].to(device)  \n","attention_mask = (input_ids != 0).to(device)  \n","pretrain_model_output = pretrain_network((input_ids, attention_mask))\n","print(pretrain_model_output.shape)\n","\n","### END YOUR CODE\n","pretrain_model_output.shape\n"]},{"cell_type":"markdown","metadata":{"id":"zik6v4lIFtrH"},"source":["**QUESTION:**\n","\n","1.a. What do the numbers above refer to?\n","\n","**ANSWER:**\n","\n","1.a. Batch Size, Sequence Lenght and Vocabulary Size"]},{"cell_type":"markdown","metadata":{"id":"ygyfBr_2FtrH"},"source":["Next, we will calculate the initial perplexity for the test set of the movie plot summaries. We need the loss function for this. Please use the cross entropy to define the loss function *loss_fn*."]},{"cell_type":"code","execution_count":14,"metadata":{"id":"uSQ_n6vDRzyx"},"outputs":[],"source":["\"\"\"\n","Define the loss function loss_fn as the cross entropy and validate/report on the\n","calculation for the average loss for the two examples\n","\n","example 1:\n","  label: 0\n","  logits: [-3.1, -2.4]\n","example 2:\n","  label: 1\n","  logits: [2.4, -3.1]\n","\n","\"\"\"\n","\n","### YOUR CODE HERE\n","\n","loss_fn = nn.CrossEntropyLoss()\n","\n","example_1_loss = loss_fn(torch.tensor([[-3.1, -2.4]]), torch.tensor([0]))\n","example_2_loss = loss_fn(torch.tensor([[2.4, -3.1]]), torch.tensor([1]))\n","average_loss = (example_1_loss + example_2_loss) / 2\n","\n","### END YOUR CODE\n","\n","\n","#loss_fn(test_input,test_target)\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(1.1032)\n","tensor(5.5041)\n","tensor(3.3036)\n"]}],"source":["# What is the average loss for these two examples?    \n","\n","print(example_1_loss)\n","print(example_2_loss)\n","print(average_loss)"]},{"cell_type":"markdown","metadata":{"id":"a0_QsbCVDGcU"},"source":["**QUESTION:**\n","\n","1.b. What is the average loss for these two examples?    \n","1.c. (Ideally, by just looking at the labels and logits), which example contributes the higher loss? Choose from 'first' or 'second'.\n","\n","**ANSWER:**\n","\n","1.b. 3.3036 \n","\n","1.c. second"]},{"cell_type":"markdown","metadata":{"id":"OxR6qSGOFtrI"},"source":["Also, consider https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html and investigate the dimensions of the input! We will need to reshape the output and the labels, because CrossEntropy expects a tensor of individual decisions, not a tensor of decision sequences. Similar for the labels. Recall how to reshape from the previous notebook.\n","\n","For the example data we calculate the loss. Then you need to calculate the perplexity."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"XGgN9KjUMkWQ"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of reshaped outputs:  torch.Size([396, 50257])\n","Shape of reshaped labels:  torch.Size([396])\n"]}],"source":["# Reshape pretrain_model_output and example_data['labels']. Name them reshaped_pretrain_model_output and reshaped_pretrain_model_labels\n","\n","### YOUR CODE HERE\n","\n","reshaped_pretrain_model_output = pretrain_model_output.view(batch_size * (max_len - 1), -1)\n","reshaped_pretrain_model_labels = example_data['labels'].view(batch_size * (max_len - 1))\n","\n","### END YOUR CODE\n","\n","print('Shape of reshaped outputs: ', reshaped_pretrain_model_output.shape)\n","print('Shape of reshaped labels: ', reshaped_pretrain_model_labels.shape)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"u_AaKxpYFtrI"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initial batch loss:  tensor(3.8828, grad_fn=<NllLossBackward0>)\n","Initial batch perplexity:  48.559882062318394\n"]}],"source":["# Now use the loss function loss_fn to first calculate - for this batch - the loss and then the perplexity.\n","\n","### YOUR CODE HERE\n","\n","initial_batch_loss = loss_fn(reshaped_pretrain_model_output, reshaped_pretrain_model_labels)\n","initial_batch_perplexity = np.exp(initial_batch_loss.cpu().detach().item())\n","\n","### END YOUR CODE\n","\n","print('Initial batch loss: ', initial_batch_loss)\n","print('Initial batch perplexity: ', initial_batch_perplexity)"]},{"cell_type":"markdown","metadata":{"id":"bQpSH7TUFtrI"},"source":["**QUESTION:**\n","\n","1.d. What is the perplexity of this batch before the training?"]},{"cell_type":"markdown","metadata":{"id":"RLSOPMqLFtrI"},"source":["Next, we will calculate the perplexity of the whole test set. For that we will use the perplexity function defined at the outset:"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"imll7TlSrRmY"},"outputs":[{"name":"stdout","output_type":"stream","text":["Current batch:  0\n","Current batch:  100\n","Current batch:  200\n","Current batch:  300\n","Current batch:  400\n","Current batch:  500\n","Current batch:  600\n","Current batch:  700\n","Current batch:  800\n","Current batch:  900\n","CPU times: user 1h 38min 49s, sys: 3min 26s, total: 1h 42min 15s\n","Wall time: 14min 31s\n"]},{"data":{"text/plain":["131019.2"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","\n","test_movie_plot_perplexity_before =  perplexity(test_texts, pretrain_network)\n","test_movie_plot_perplexity_before"]},{"cell_type":"markdown","metadata":{"id":"QTBvxzNwksC8"},"source":["**QUESTION**:\n","\n","1.e. What is the perplexity of the test set before further training?"]},{"cell_type":"markdown","metadata":{"id":"ABTv8wdBQfhz"},"source":["Ok. What about random text not from this domain? Let us look at the random snippets from Hugging Face blogs defined above in random_huggingface_blog_text:"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"0Sq1xBa3JMBv"},"outputs":[{"name":"stdout","output_type":"stream","text":["Current batch:  0\n"]},{"data":{"text/plain":["236230.06"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["test_hf_perplexity_before = perplexity(fake_data_loader(random_huggingface_blog_text, tokenizer=gpt_2_tokenizer, max_len=100), pretrain_network)\n","test_hf_perplexity_before"]},{"cell_type":"markdown","metadata":{"id":"bLLMsPPtFtrI"},"source":["Good, about the same. (As hoped/expected. The model should not have any particular better understanding for either type of text.)\n","\n","Next, we need to create the optimizer and generate a training loop. Nothing to do here for you, but take a look if interested."]},{"cell_type":"code","execution_count":20,"metadata":{"id":"HQQdjKecFtrI"},"outputs":[],"source":["pretrain_optimizer = torch.optim.AdamW(pretrain_network.parameters(), lr=0.00001)\n","\n","def continued_train_loop(dataloader,\n","               model,\n","               loss_fn,\n","               optimizer,\n","               reporting_interval=100,\n","               max_len=100,\n","               steps=None):\n","\n","    \"\"\"\n","    Write the training loop for continued pre-training. In particular, you need to:\n","    - initialize the epoch_loss to 0 and set the model into training mode\n","    - iterate over the batches:\n","      - break if you are at 'steps' number of batches\n","      - get the inputs X and labels y (which in this case will be the actual next token)\n","      - get the model outputs\n","      - reshape y and model outputs in proper format for cross entropy calculation\n","      - zero out the gradient\n","      - calculate loss\n","      - propagate loss (loss.backward) and apply optimizer step\n","      - add the loss to the epoch_loss\n","    Reporting:\n","      - report the current average loss and perplexity every 'reporting_interval' batches\n","      - report the average loss and perplexity at the end of the epoch (done for you)\n","\n","    \"\"\"\n","\n","\n","    # Set the model to training mode - important for batch normalization and dropout layers\n","    # Unnecessary in this situation but added for best practices\n","\n","    ### YOUR CODE HERE\n","\n","    model.train()\n","    epoch_loss = 0\n","    batch_count = 0 \n","\n","    for step, batch_data in enumerate(dataloader):\n","      if steps and step >= steps:\n","        break  # Stop training after a certain number of steps\n","\n","        batch_input = batch_data['input'].to(device)\n","        batch_labels = batch_data['labels'].to(device)\n","\n","        # Forward pass: get model outputs\n","        batch_output = model(batch_input)\n","        \n","        # Reshape model outputs and labels for cross-entropy loss\n","        batch_output_reshaped = batch_output.view(-1, batch_output.size(-1))\n","        batch_labels_reshaped = batch_labels.view(-1)\n","\n","        # Zero out gradients\n","        optimizer.zero_grad()\n","\n","        # Compute loss\n","        batch_loss = loss_fn(batch_output_reshaped, batch_labels_reshaped)\n","\n","        # Backpropagate loss and apply optimizer step\n","        batch_loss.backward()\n","        optimizer.step()\n","\n","        # Add batch loss to epoch loss\n","        epoch_loss += batch_loss.item()\n","        batch_count += 1\n","\n","        # Report progress at intervals\n","        if step % reporting_interval == 0 and step > 0:\n","            avg_loss = epoch_loss / batch_count\n","            perplexity = np.exp(avg_loss)\n","            print(f\"Step {step}, Avg train loss: {avg_loss:.6f}, Avg perplexity: {perplexity:.6f}\")\n","\n","    # Handle the case where no batches were processed\n","    if batch_count == 0:\n","      print(\"No batches were processed.\")\n","      return\n","\n","    # At the end of the epoch, report the average loss and perplexity\n","    avg_epoch_loss = epoch_loss / batch_count\n","    avg_perplexity = np.exp(avg_epoch_loss)\n","    \n","\n","    ### END YOUR CODE\n","\n","    print(f\"Training Results: \\n  Avg train loss: {epoch_loss/batch:>8f} \\n Avg train perplexity: {np.exp(epoch_loss/batch):>8f} \")"]},{"cell_type":"markdown","metadata":{"id":"FBuMQv3-FtrI"},"source":["Now we do the training:"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"9DahA3McFtrI"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1\n","-------------------------------\n","No batches were processed.\n","Done!\n"]}],"source":["epochs = 1\n","for t in range(epochs):\n","    # we just train for 1000 batches\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    continued_train_loop(train_texts, pretrain_network, loss_fn, pretrain_optimizer, steps=1000)\n","\n","print(\"Done!\")"]},{"cell_type":"markdown","metadata":{"id":"Vq71uMOeFtrI"},"source":["How did the perplexity of the test set change after the additional pre-training?"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"1YAz5ORhFtrJ"},"outputs":[{"name":"stdout","output_type":"stream","text":["Current batch:  0\n","Current batch:  100\n","Current batch:  200\n","Current batch:  300\n","Current batch:  400\n","Current batch:  500\n","Current batch:  600\n","Current batch:  700\n","Current batch:  800\n","Current batch:  900\n","CPU times: user 1h 48min 18s, sys: 4min 46s, total: 1h 53min 4s\n","Wall time: 16min 52s\n"]},{"data":{"text/plain":["63214.67"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","\n","test_movie_plot_perplexity_after = perplexity(test_texts, pretrain_network)\n","test_movie_plot_perplexity_after"]},{"cell_type":"markdown","metadata":{"id":"zY0Ri85JRWht"},"source":["What about the Hugging Face blog snippets that were not in the movie domain:\n","\n"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"wQfYKFVgRW1w"},"outputs":[{"name":"stdout","output_type":"stream","text":["Current batch:  0\n"]},{"data":{"text/plain":["115401.84"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["test_hf_perplexity_after =  perplexity(fake_data_loader(random_huggingface_blog_text, tokenizer=gpt_2_tokenizer, max_len=100), pretrain_network)\n","test_hf_perplexity_after"]},{"cell_type":"markdown","metadata":{"id":"0SgmygRcuVa5"},"source":["\n","**QUESTION:**\n","\n","1.f. What is your observation about the perplexity change for the test movie plot set texts after the additional pre-training? About the same ('within 2'), higher, or lower?\n","\n","1.g. What is your observation about the perplexity change for the Hugging Face texts after the additional pre-training? About the same ('within 2'), higher, or lower?\n","\n","1.h. (Free form) What would these observations imply in terms of where/how this model could be used?"]},{"cell_type":"markdown","metadata":{"id":"l9VdYiNgw_jm"},"source":["## 2. Sentiment Classification of the IMDB Movie dataset using GPT2 and Prompts\n","\n","We will now get the IMDB dataset, just like we did in the PyTorch Intro II notebook. Refer to it for details."]},{"cell_type":"code","execution_count":24,"metadata":{"id":"yU4fUhe6BEJZ"},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]}],"source":["imdb_dataset = load_dataset(\"IMDB\")"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"zF3CSiIKD752"},"outputs":[{"data":{"text/plain":["{'text': 'It was great to see some of my favorite stars of 30 years ago including John Ritter, Ben Gazarra and Audrey Hepburn. They looked quite wonderful. But that was it. They were not given any characters or good lines to work with. I neither understood or cared what the characters were doing.<br /><br />Some of the smaller female roles were fine, Patty Henson and Colleen Camp were quite competent and confident in their small sidekick parts. They showed some talent and it is sad they didn\\'t go on to star in more and better films. Sadly, I didn\\'t think Dorothy Stratten got a chance to act in this her only important film role.<br /><br />The film appears to have some fans, and I was very open-minded when I started watching it. I am a big Peter Bogdanovich fan and I enjoyed his last movie, \"Cat\\'s Meow\" and all his early ones from \"Targets\" to \"Nickleodeon\". So, it really surprised me that I was barely able to keep awake watching this one.<br /><br />It is ironic that this movie is about a detective agency where the detectives and clients get romantically involved with each other. Five years later, Bogdanovich\\'s ex-girlfriend, Cybil Shepherd had a hit television series called \"Moonlighting\" stealing the story idea from Bogdanovich. Of course, there was a great difference in that the series relied on tons of witty dialogue, while this tries to make do with slapstick and a few screwball lines.<br /><br />Bottom line: It ain\\'t no \"Paper Moon\" and only a very pale version of \"What\\'s Up, Doc\".',\n"," 'label': 0}"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["imdb_dataset['train'][10]"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"S5UaatHFEjEK"},"outputs":[{"name":"stdout","output_type":"stream","text":["positive:  12500\n","negative:  12500\n","other:  0\n","positive:  12500\n","negative:  12500\n","other:  0\n"]}],"source":["imdb_train_set = create_temp_set(imdb_dataset['train'])\n","imdb_test_set = create_temp_set(imdb_dataset['test'])"]},{"cell_type":"markdown","metadata":{"id":"XOzsNC6aFtrJ"},"source":["Usually we would first get the Dataset and the Dataloader, however for reasons that hopefully become apparent, in this case we first want to build the network. You should think of the network as taking input_ids $x$ from a batch of suitably tokenized sentences, and the **output should be the logits of the last token for each example in the batch.**\n","\n","Please fill in the missing line:"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"ZGSGslw1BJtk"},"outputs":[],"source":["%%capture\n","\n","class TokenPredictionNetworkClass(torch.nn.Module):\n","    \"\"\"\n","    Define a simple PyTorch network that takes a batch (from a Dataloader)\n","    as input and returns the logits for the last next-token prediction.\n","    When instantiated, you need to pass in a pretrained base language model\n","    (the 'logit_model').\n","    You need to define both, the __init__ and the forward methods.\n","    \"\"\"\n","\n","    def __init__(self, logit_model):\n","        ### YOUR CODE HERE\n","\n","        super(TokenPredictionNetworkClass, self).__init__()\n","        self.logit_model = logit_model\n","\n","        ### END YOUR CODE\n","\n","    def forward(self, x):\n","        # get the logits for the last position of each each example. (Call them last_token_logits.). This will be just one line.\n","        # Use self.logit_model(x) to get the model output\n","        ### YOUR CODE HERE\n","\n","        last_token_logits = self.logit_model(x).logits[:, -1, :]\n","\n","        ### END YOUR CODE\n","\n","        return last_token_logits\n","\n","\n","loss_fn = torch.nn.CrossEntropyLoss()\n","\n","token_prediction_network_base_model = TokenPredictionNetworkClass(logit_model=base_gpt2_model)\n","token_prediction_network_addnl_pretrain_model = TokenPredictionNetworkClass(logit_model=additinal_pretrain_gpt2_model)\n","\n","token_prediction_network_base_model.to(device)\n","token_prediction_network_addnl_pretrain_model.to(device)"]},{"cell_type":"markdown","metadata":{"id":"Dad6GNLWBdvb"},"source":["We now construct our training and test sets for the Sentiment Classification. We want to follow a different approach than we have in the PyTorch intro II notebook. We want to leverage the language model and what it is good at - predicting the next tokens - to the maximum. So why not put a 'wrapper' around the review in a way that the proper sentiment would be naturally the next word?\n","\n","As a simple example, rather than trying to use the last output vector and add a classification layer let's try to reframe the problem like this (as an illustrative example):\n","\n","  \n"," \"This is a review: <truncated review text>... The reviewer classifies reviews as good or bad. In this case they thought the movie was\"\n","\n"," or\n","\n"," \"This is a review: <truncated review text>... The reviewer has positive or negative sentiments about movies. In this case the sentiment was\"\n","\n"," ...\n","\n"," One would think that the LM should already do a decent job getting the proper sentiment simply using the next word prediction task it is trained on!\n","\n"," How could we test this? We could simply consider the cross entropy loss for the next token relative to the actual sentiment, i.e. the next word we would expect for a positive or a negative review.\n","\n"," So we can experient with:\n","\n"," * The pre-fix before the review\n"," * The text after the review\n"," * The words we would expect for pos/neg reviews\n","\n","Try a few combinations and see which ones give you the lowest loss.\n","\n","**NOTE:** Usually we also would do a good chunk of text pre-processing (take out html, etc.), but for simplicity we will ignore this."]},{"cell_type":"code","execution_count":28,"metadata":{"id":"o0yNQL1WFuJu"},"outputs":[],"source":["class ClassificationData(Dataset):\n","    def __init__(self,\n","                 base_data,\n","                 tokenizer,\n","                 max_len,\n","                 use_prompt=False,\n","                 prompt_pre_text='',\n","                 prompt_post_text='',\n","                 classification_tokenset={1: 'good', 0: 'bad'},\n","                 num_examples=-1):\n","\n","        self.max_len = max_len\n","        self.tokenizer = tokenizer  # assume that padding token has already been added to tokenizer\n","        self.data = []\n","\n","        # really  not ideal having to iterate through the whole set. But ok for this small data volume\n","\n","\n","\n","        for num_example, example in enumerate(base_data):\n","\n","            if num_examples != -1 and num_example >= num_examples:\n","              break\n","\n","            if num_example == 0:\n","              print(example)\n","\n","\n","            token_encoder = self.tokenizer(example['text'])['input_ids']\n","\n","            if len(token_encoder) <= self.max_len:\n","                continue    # avoids complications with short sentences. No padding is needed then.\n","\n","            truncated_encoding = token_encoder[:self.max_len]\n","            truncated_example = tokenizer.decode(truncated_encoding) # reconstruct shortened review\n","\n","            # LLMs do next-word predictions. You may want to add a prompt that the model can work with!\n","\n","\n","            if use_prompt:\n","\n","                additional_token_length = len(self.tokenizer(prompt_pre_text)['input_ids']) + len(self.tokenizer(prompt_post_text)['input_ids'])\n","                cutoff = self.max_len + additional_token_length - 1\n","\n","                prompted_text_line = prompt_pre_text + truncated_example + prompt_post_text\n","\n","            else:\n","                cutoff = self.max_len\n","                prompted_text_line = truncated_example\n","\n","            if len(self.tokenizer(prompted_text_line)['input_ids']) != cutoff:\n","                    continue\n","\n","\n","\n","            tokenized_example = self.tokenizer(prompted_text_line,\n","                                               return_tensors=\"pt\",\n","                                               max_length=cutoff,\n","                                               truncation=True,\n","                                               padding='max_length').to(device)\n","\n","            #if num_example == 0:\n","            #  print(self.tokenizer.decode(tokenized_example['input_ids'][0]))\n","\n","            if example['label'] == 1:\n","              token = classification_tokenset[1]\n","            else:\n","              token = classification_tokenset[0]\n","\n","            token_id = self.tokenizer.encode(' ' + token)[0]\n","            label = torch.tensor(token_id, dtype=torch.int64, device=device)\n","\n","            self.data.append({'label': label,\n","                              'input_ids': torch.squeeze(tokenized_example['input_ids']).to(device)\n","                              })\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","\n","        return {\n","            'input_ids': self.data[index]['input_ids'],\n","            'label': self.data[index]['label']\n","        }"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"lIavTztxFtrJ"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'text': \"Normally I would never rent a movie like this, because you know it's going to be bad just by looking at the box. I rented seven movies at the same time, including Nightmare on Elm Street 5, 6 and Wes Craven's New Nightmare. Unfortunately, when I got home I found out the videostore-guy gave me the wrong tape. In the box of Wes Craven's New Nightmare I found this lame movie.<br /><br />This movie is incredibly boring, the acting is bad and the plot doesn't make any sense. It's hard to write a good review, because I have no idea what the movie was really about. At the end of the movie you have more questions then answers.<br /><br />On 'Max Power's Scale of 1 to 10' I rate this movie: 1<br /><br />PS I would like to correct Corinthian's review (right below mine). He says Robert Englund is ripping off lingerie, riding horses naked, etc. The guy that did those things was Mahmoud, played by Juliano Mer, not by Robert Englund.\", 'label': 0}\n"]},{"name":"stderr","output_type":"stream","text":["2024-10-02 19:09:16.626694: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]},{"name":"stdout","output_type":"stream","text":["Average loss:  tensor(2.0389)\n","Predicted tokens vs labels:  [('good', 'bad'), ('good', 'good'), ('good', 'good'), ('good', 'good'), ('good', 'bad'), ('good', 'good'), ('good', 'good'), ('good', 'good'), ('good', 'good'), ('a', 'good'), ('good', 'good'), ('good', 'bad'), ('bad', 'bad'), ('good', 'good'), ('good', 'good'), ('good', 'good'), ('a', 'bad'), ('good', 'bad'), ('good', 'bad')]\n"]}],"source":["#Suggested, but try a bunch!\n","# prompt_pre_text = 'Here is a movie review: '\n","#prompt_post_text = ' ...  The reviewer classifies reviews as good or bad. In this case they thought the movie was'\n","#classification_tokenset = {1: 'good', 0: 'bad'}\n","\n","prompt_pre_text = 'Here is a movie review: '\n","prompt_post_text = ' ...  The reviewer classifies reviews as good or bad. In this case they thought the movie was'\n","classification_tokenset = {1: 'good', 0: 'bad'}\n","\n","# make a modification to the prompt_pre_text, prompt_post_text, and classification_tokenset\n","# that gets the loss below 1.7\n","\n","### YOUR CODE HERE\n","\n","prompt_pre_text = 'Here is a movie review: '\n","prompt_post_text = ' ...  The reviewer classifies reviews as good or bad. In this case they thought the movie was'\n","classification_tokenset = {1: 'good', 0: 'bad'}\n","\n","### END YOUR CODE\n","\n","play_data = ClassificationData(imdb_train_set,\n","                                tokenizer=gpt_2_tokenizer,\n","                                max_len=100,\n","                                use_prompt=True,\n","                                prompt_pre_text = prompt_pre_text,\n","                                prompt_post_text = prompt_post_text,\n","                                classification_tokenset=classification_tokenset,\n","                                num_examples=20\n","                                )\n","\n","batch_size = 4\n","toy_texts = DataLoader(play_data, batch_size=batch_size, shuffle=True)\n","\n","\n","loss = 0\n","predicted_tokens = []\n","labels = []\n","\n","for batch_num, toy_text_batch in enumerate(toy_texts):\n","    sample_output = token_prediction_network_base_model(toy_text_batch['input_ids']).to(device)\n","    sample_labels = toy_text_batch['label']\n","    loss += loss_fn(sample_output, sample_labels).detach()\n","\n","    predicted_tokens += gpt_2_tokenizer.decode(torch.argmax(sample_output, dim=-1)).split()\n","    labels += gpt_2_tokenizer.decode(sample_labels).split()\n","\n","loss /= (batch_num + 1)\n","\n","\n","\n","print('Average loss: ', loss)\n","print('Predicted tokens vs labels: ', [(x, y) for x,y in zip(predicted_tokens, labels)])\n"]},{"cell_type":"markdown","metadata":{"id":"vB_NC-2yFtrJ"},"source":["Ok, the accuracy using the old GPT2 is not exactly amazing (newer and larger models would be MUCH better out of the box). However, even for GPT2 at least a token of the right type is predicted. Fine-tuning should make this much better!\n","\n","\n","**QUESTION:**\n","\n","2.a. Write down two different prompt_pre_text/prompt_post_text combinations and their respective average loss. Pick one that sounds reasonable but is quite a bit worse (say, average loss > 3), and another that gets the loss below 1.7. (Note, for the later you probably havde to counteract a bit the model's tendency to be positve. You may also want to be more clear where the review\n","starts and ends.)\n","\n","\n","Now let's do the fine-tuning that is supposed to help! Start by getting the full dataset and dataloaders:"]},{"cell_type":"code","execution_count":30,"metadata":{"id":"6seAp7dmFtrJ"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'text': \"Normally I would never rent a movie like this, because you know it's going to be bad just by looking at the box. I rented seven movies at the same time, including Nightmare on Elm Street 5, 6 and Wes Craven's New Nightmare. Unfortunately, when I got home I found out the videostore-guy gave me the wrong tape. In the box of Wes Craven's New Nightmare I found this lame movie.<br /><br />This movie is incredibly boring, the acting is bad and the plot doesn't make any sense. It's hard to write a good review, because I have no idea what the movie was really about. At the end of the movie you have more questions then answers.<br /><br />On 'Max Power's Scale of 1 to 10' I rate this movie: 1<br /><br />PS I would like to correct Corinthian's review (right below mine). He says Robert Englund is ripping off lingerie, riding horses naked, etc. The guy that did those things was Mahmoud, played by Juliano Mer, not by Robert Englund.\", 'label': 0}\n"]},{"name":"stderr","output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (1038 > 1024). Running this sequence through the model will result in indexing errors\n"]},{"name":"stdout","output_type":"stream","text":["{'text': \"I know most of the other reviews say that this movie was great, but I have to disagree.<br /><br />Sure, it's a good book! It was actually one of my favorites when I was verrry little. But it's just not meant for theaters. Maybe for a little half-hour short, but I don't see how they can turn a short kiddie book into a whole feature film.<br /><br />It is a cute movie, but I would only recommend it for really little kids. Older kids will have no interest it. Adults may have a little more interest if they watch it with their young ones. But anyone ages 7-Adult will have a snore-fest.<br /><br />Sorry if you disagree with me, but this is my opinion. :)\", 'label': 0}\n"]}],"source":["imdb_train_data = ClassificationData(imdb_train_set,\n","                                tokenizer=gpt_2_tokenizer,\n","                                max_len=100,\n","                                use_prompt=True,\n","                                prompt_pre_text = prompt_pre_text,\n","                                prompt_post_text = prompt_post_text,\n","                                classification_tokenset=classification_tokenset,\n","                                num_examples=-1\n","                                )\n","\n","imdb_test_data = ClassificationData(imdb_test_set,\n","                                tokenizer=gpt_2_tokenizer,\n","                                max_len=100,\n","                                use_prompt=True,\n","                                prompt_pre_text = prompt_pre_text,\n","                                prompt_post_text = prompt_post_text,\n","                                classification_tokenset=classification_tokenset,\n","                                num_examples=-1\n","                                )\n","\n","\n","batch_size = 4\n","imdb_train_loader = DataLoader(imdb_train_data, batch_size=batch_size, shuffle=True)\n","imdb_test_loader = DataLoader(imdb_test_data, batch_size=batch_size, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"6lMTx6IUMFyv"},"source":["Let's set up the optimizers as before:"]},{"cell_type":"code","execution_count":31,"metadata":{"id":"Zer3mvWGK89C"},"outputs":[],"source":["adam_optimizer_base_model = torch.optim.AdamW(token_prediction_network_base_model.parameters(), lr=0.00001)\n","adam_optimizer_addtl_pretrain_model = torch.optim.AdamW(token_prediction_network_addnl_pretrain_model.parameters(), lr=0.00001)"]},{"cell_type":"markdown","metadata":{"id":"wA2F_LP-5Wqe"},"source":["Here is the new training loop. Please fill in the lines for optimizer zeroing, the prediction calculation, and the loss."]},{"cell_type":"code","execution_count":32,"metadata":{"id":"a5B2VrbX1uLN"},"outputs":[],"source":["def train_loop(dataloader, model, loss_fn, optimizer, reporting_interval=100, steps=None):\n","\n","    \"\"\"\n","    Write the training loop to fine-tune the model for sentiment\n","    classification using the final next-token-prediction task.\n","    In particular, you need to:\n","    - initialize the epoch_loss to 0 and set the model into training mode\n","    - iterate over the batches:\n","      - break if you are at 'steps' number of batches\n","      - get the inputs X and labels y (which in this case will be the actual next token)\n","      - get the model outputs\n","      - reshape y and model outputs in proper format for cross entropy calculation\n","      - zero out the gradient\n","      - calculate loss\n","      - propagate loss (loss.backward) and apply optimizer step\n","      - add the loss to the epoch_loss\n","    Reporting:\n","      - report the current average loss every 'reporting_interval' batches\n","      - report the average loss at the end of the epoch (done for you)\n","\n","    \"\"\"\n","\n","    ### YOUR CODE HERE\n","\n","    model.train()\n","    epoch_loss = 0\n","    batch_count = 0\n","\n","    for step, batch_data in enumerate(dataloader):\n","      if steps and step >= steps:\n","        break\n","\n","      batch_input = batch_data['input_ids'].to(device)\n","      batch_labels = batch_data['label'].to(device)\n","\n","      batch_output = model(batch_input)\n","\n","      batch_output_reshaped = batch_output.view(-1, batch_output.size(-1))\n","      batch_labels_reshaped = batch_labels.view(-1)\n","\n","      optimizer.zero_grad()\n","\n","      batch_loss = loss_fn(batch_output_reshaped, batch_labels_reshaped)\n","\n","      batch_loss.backward()\n","      optimizer.step()\n","\n","      epoch_loss += batch_loss.item()\n","      batch_count += 1\n","\n","      if step % reporting_interval == 0 and step > 0:\n","          avg_loss = epoch_loss / batch_count\n","          print(f\"Step {step}, Avg train loss: {avg_loss:.6f}\")\n","\n","    if batch_count == 0:\n","      print(\"No batches were processed.\")\n","      return\n","    \n","    avg_epoch_loss = epoch_loss / batch_count\n","\n","    ### END YOUR CODE\n","\n","    print(f\"Training Results: \\n  Avg train loss: {epoch_loss/batch:>8f} \\n\")\n","\n","\n","def test_loop(dataloader, model, loss_fn, reporting_interval=100, contrast_pair=None,steps=None):\n","    \"\"\"\n","    Write the test loop to fine-tune the model for sentiment classification using the final next-token-prediction task.\n","    In particular, you need to:\n","\n","    - set the model into eval mode and initialize the test_loss to 0. Also, set the number of correct &\n","      total test examples to 0, like:\n","      'test_loss, correct_token_predictions,  correct_label_class, total = 0, 0, 0, 0'\n","        (See the two approaches for accuracy below for correct_token_predictions\n","        and correct_label_class)\n","\n","    - use torch.no_grad to iterate over the batches:\n","        - break if you are at 'steps' number of batches\n","        - from the batch, get the test inputs X and labels y (which in this case will be the actual next token). You may want to look at the format of batches by using 'next(iter(imdb_test_loader))' in a separate cell\n","        - get the model outputs\n","        - calculate loss and add to test_loss (reshaping should not be necessary)\n","        - For the accuracy, we can try two approaches (and in this case they should turn out to be\n","            probably the same in the end):\n","              i) Test Class Accuracy:\n","                  - Define the predicted class (I call it selected class) by comparing the logits for our two\n","                    'evaluating tokens' (like 'good', 'bad'). if the logit for (in this example) 'good' is higher,\n","                    then the predicted class is the positive one, etc.\n","              ii)  Token Prediction Accuracy:\n","                  - see how often the correct 'evaluation token' is predicted. I.e., here we do not compare\n","                    whether the model believes that 'good' is a more likely next token than 'bad', but was\n","                    'good' the actual next token prediction (and vise versa).\n","              - get these numbers for each batch and add to the totals\n","\n","    - add the loss to the epoch_loss\n","\n","    Reporting:\n","\n","    - report on the average token accuracy, average class accuracy and average test loss every 'reporting_interval' batches\n","    - report on the same at the end (done for you)\n","\n","    \"\"\"\n","\n","    # let's get the proper class ids for the 'evaluating next tokens' (like 'good', 'bad')\n","\n","    if contrast_pair is not None:\n","      class_1, class_2 = contrast_pair\n","      class_1_id, class_2_id = gpt_2_tokenizer.encode(' ' + class_1 + ' ' + class_2)\n","\n","    # now the loop starts:\n","\n","    ### YOUR CODE HERE\n","\n","    model.eval()\n","\n","    test_loss, correct_token_predictions, correct_label_class, total = 0, 0, 0, 0\n","\n","    for step, batch_data in enumerate(dataloader):\n","      if steps and step >= steps:\n","        break\n","\n","      batch_input = batch_data['input_ids'].to(device)\n","      batch_labels = batch_data['label'].to(device)\n","\n","      batch_output = model(batch_input)\n","\n","      batch_loss = loss_fn(batch_output, batch_labels)\n","      test_loss += batch_loss.item()\n","\n","      for i in range(batch_output.size(0)):\n","        predicted_class = torch.argmax(batch_output[i]).item()\n","        correct_class = batch_labels[i].item()\n","\n","        if predicted_class == correct_class:\n","          correct_label_class += 1\n","\n","        if contrast_pair:\n","          if predicted_class == class_1_id:\n","            predicted_token = class_1\n","          elif predicted_class == class_2_id:\n","            predicted_token = class_2\n","          else:\n","            predicted_token = 'unknown'\n","\n","          if correct_class == class_1_id:\n","            correct_token = class_1\n","          elif correct_class == class_2_id:\n","            correct_token = class_2\n","          else:\n","            correct_token = 'unknown'\n","\n","          if predicted_token == correct_token:\n","            correct_token_predictions += 1\n","\n","        else:\n","          if predicted_class == correct_class:\n","            correct_token_predictions += 1\n","\n","        total += 1\n","\n","    avg_test_loss = test_loss / total\n","    avg_token_accuracy = correct_token_predictions / total\n","    avg_class_accuracy = correct_label_class / total\n","\n","    ### END YOUR CODE\n","\n","    print(correct_label_class)\n","    print(f\"Test Results: \\n\\t Test Token Accuracy: {(100*correct):>0.1f}% \\n\\t Test Class Accuracy: {(100*correct_label_class):>0.1f}%  \\n\\t Avg test loss: {test_loss:>8f} \\n\")"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"tfYYfoCK1uOX"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1\n","-------------------------------\n","Step 100, Avg train loss: 0.691266\n","Step 200, Avg train loss: 0.600858\n","Step 300, Avg train loss: 0.556341\n","Step 400, Avg train loss: 0.538435\n","Step 500, Avg train loss: 0.515655\n","Step 600, Avg train loss: 0.507377\n","Step 700, Avg train loss: 0.499678\n","Step 800, Avg train loss: 0.493247\n","Step 900, Avg train loss: 0.488447\n","Step 1000, Avg train loss: 0.479650\n","Step 1100, Avg train loss: 0.467019\n","Step 1200, Avg train loss: 0.463335\n","Step 1300, Avg train loss: 0.457502\n","Step 1400, Avg train loss: 0.451654\n","Step 1500, Avg train loss: 0.446935\n","Step 1600, Avg train loss: 0.442717\n","Step 1700, Avg train loss: 0.440059\n","Step 1800, Avg train loss: 0.433151\n","Step 1900, Avg train loss: 0.430274\n"]},{"ename":"NameError","evalue":"name 'batch' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[33], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimdb_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_prediction_network_base_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madam_optimizer_base_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     test_loop(imdb_test_loader, token_prediction_network_base_model, loss_fn, contrast_pair\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgood\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbad\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      6\u001b[0m               steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m\n\u001b[1;32m      7\u001b[0m               ) \u001b[38;5;66;03m# no optimizer use here!\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[32], line 63\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer, reporting_interval, steps)\u001b[0m\n\u001b[1;32m     59\u001b[0m avg_epoch_loss \u001b[38;5;241m=\u001b[39m epoch_loss \u001b[38;5;241m/\u001b[39m batch_count\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m### END YOUR CODE\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Results: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m  Avg train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_loss\u001b[38;5;241m/\u001b[39mbatch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m>8f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"]}],"source":["epochs = 1\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train_loop(imdb_train_loader, token_prediction_network_base_model, loss_fn, adam_optimizer_base_model, steps=2000)\n","    test_loop(imdb_test_loader, token_prediction_network_base_model, loss_fn, contrast_pair=('good', 'bad'),\n","              steps=500\n","              ) # no optimizer use here!\n","print(\"Done!\")"]},{"cell_type":"markdown","metadata":{"id":"cxWvZSg4WAzj"},"source":["Now we redo this for the model that saw the additional pre-training:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O20xrf0gmscR"},"outputs":[],"source":["epochs = 1\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train_loop(imdb_train_loader, token_prediction_network_addnl_pretrain_model, loss_fn, adam_optimizer_addtl_pretrain_model, steps=2000)\n","    test_loop(imdb_test_loader, token_prediction_network_addnl_pretrain_model, loss_fn, contrast_pair=('good', 'bad'),\n","              steps=500\n","              ) # no optimizer use here!\n","print(\"Done!\")"]},{"cell_type":"markdown","metadata":{"id":"221hDSFSaiee"},"source":["This looks good! So we had better movie review sentiment classification using the model had had seen the additional pretraining.\n","\n","**QUESTION:**\n","\n","2.b. What was your test accuracy after fine-tuning, when starting with the base model?\n","\n","2.c. What was your test accuracy after fine-tuning, when starting with the model that had additional pre-training?\n","\n","2.d. Based on this and what we saw in the previous section (and, as there are statistical fluctuations, based on what 'should' be the case), what would be your expectation for these two starting models when used for sentiment analysis tasks that deal with data **inside** the movie domain? ('base model slightly better', or 'additional pretrain model slightly better')\n","\n","2.e. Based on this and what we saw in the previous section (and, as there are statistical fluctuations, based on what 'should' be the case), what would be your expectation for these two starting models when used for sentiment analysis tasks that deal with data  **outside** the movie domain? ('base model slightly better', or 'additional pretrain model slightly better')"]},{"cell_type":"markdown","metadata":{"id":"UyUulgoDuf0_"},"source":["## 3. Sentiment Classification with BERT\n","\n","Now we will see how well the classification with BERT works in comparison. We will get the model tokenizer for that model, then - as discussed in class - use the output of the initial [CLS] token to classify the sentiment. Will it be better? Or worse?\n","\n","See https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertModel for more details around the model.\n","\n"]},{"cell_type":"code","execution_count":34,"metadata":{"id":"vU5Aa6nY1vrM"},"outputs":[],"source":["%%capture\n","\n","from transformers import AutoTokenizer, BertModel\n","\n","\n","bert_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n","bert_model = BertModel.from_pretrained(\"bert-base-cased\").to(device)"]},{"cell_type":"markdown","metadata":{"id":"ZrILbM7Tuf0_"},"source":["Let us look at a simple bert tokenization"]},{"cell_type":"code","execution_count":35,"metadata":{"id":"d63Uoyx_gbiP"},"outputs":[{"data":{"text/plain":["torch.Size([1, 5, 768])"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["bert_toy_inputs = bert_tokenizer(\"This is new\", return_tensors=\"pt\").to(device)\n","bert_toy_outputs = bert_model(**bert_toy_inputs)\n","\n","last_hidden_states = bert_toy_outputs.last_hidden_state\n","\n","last_hidden_states.shape"]},{"cell_type":"markdown","metadata":{"id":"7g-wccv3STm0"},"source":["Play with decode method of the tokenizer to see why the shape is ... x 5 x ... . Then identify the first value of the output of the [CLS] token."]},{"cell_type":"code","execution_count":36,"metadata":{"id":"_W0c0wJ1Rwjf"},"outputs":[{"name":"stdout","output_type":"stream","text":["The tokens after tokenization:  [CLS] This is new [SEP]\n"]}],"source":["# Decode the tokenization. Call it bert_toy_tokens.\n","### YOUR CODE HERE\n","\n","bert_toy_tokens = bert_tokenizer.decode(bert_toy_inputs['input_ids'][0])\n","\n","### END YOUR CODE\n","print('The tokens after tokenization: ', bert_toy_tokens)"]},{"cell_type":"markdown","metadata":{"id":"I8QxOlH7XtoT"},"source":["**QUESTION:**\n","\n","3.a Why is the shape .. x 5 x ... and not .. x 3 x ... ? Explain. (You may need to to look up what the purpose is of one of the extra tokens. Don't write more than 2-3 lines.)"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"lIjgqCXdRwvL"},"outputs":[{"name":"stdout","output_type":"stream","text":["First output of [CLS] token:  tensor([ 4.7248e-01,  1.7685e-01,  5.4234e-01, -5.4906e-01, -1.1503e-01,\n","         1.1630e-01,  4.2311e-01,  2.2489e-02, -1.9720e-01, -9.8925e-01,\n","        -4.3665e-01,  3.0220e-01, -4.5189e-01,  1.0892e-01, -6.0475e-01,\n","         2.3494e-01,  1.0624e-01,  3.3463e-01,  9.6197e-02, -7.3822e-02,\n","         1.8399e-01, -2.7128e-01,  6.3856e-01, -2.2248e-01,  4.8449e-01,\n","        -2.6155e-01,  6.0228e-01,  1.4545e-01,  4.9357e-02,  4.4853e-01,\n","        -5.3474e-02,  6.7213e-02,  4.8560e-02, -6.9567e-02,  3.0451e-02,\n","        -1.2569e-01,  1.5189e-02, -6.5966e-01, -5.1185e-03, -1.5514e-01,\n","        -5.0256e-01,  2.3907e-01,  2.9280e-01, -1.1793e-01,  5.0254e-01,\n","        -7.8381e-01, -8.3310e-02, -1.2723e-01, -4.2026e-01,  1.1898e-01,\n","         7.1867e-02,  1.9298e-01, -5.5773e-02,  3.2795e-01, -5.6385e-03,\n","        -1.5043e-01, -5.4291e-01,  3.0932e-01, -5.1025e-01,  2.8927e-01,\n","         1.8317e-01, -4.7179e-03,  4.0482e-01, -2.3334e-03, -5.9042e-02,\n","        -9.0397e-02, -8.2895e-02,  1.5881e-01, -4.7864e-01, -2.8077e-01,\n","        -1.0255e-01,  1.9993e-01,  4.3991e-01,  1.0241e+00,  4.6022e-01,\n","        -2.7858e-01,  3.4326e-01,  3.9601e-01,  5.6149e-02,  8.6239e-02,\n","         2.6598e-01,  4.4734e-01, -2.9577e-02, -2.6248e-01,  9.4554e-02,\n","        -8.9575e-03,  4.1349e-01, -5.2663e-02, -1.4059e-01,  3.5523e-01,\n","         2.5458e-01, -1.1831e-01, -7.5502e-01, -2.9882e-01,  4.3147e-02,\n","         3.6292e-01,  1.7790e-02,  1.8058e-01,  6.1829e+00, -1.4481e-01,\n","         1.3347e-01, -9.4680e-02,  7.5315e-01,  5.9179e-02,  6.0372e-01,\n","        -9.9446e-02, -1.6166e-02, -1.1691e-01,  2.7946e-01,  4.6337e-01,\n","         2.8236e-01, -3.4459e-01,  9.7477e-02, -2.9536e-01, -2.5241e-01,\n","        -2.4220e-01, -6.8551e-02,  4.2699e-01,  1.9218e-01, -3.3008e-01,\n","         1.5981e-01,  1.1538e-01,  1.3907e+00,  3.4374e-01, -3.4656e-01,\n","        -1.4068e-01, -2.8949e-01,  3.7642e-02,  2.3393e-01, -3.2027e-01,\n","        -4.8399e-01, -4.5009e-01, -1.4058e-01, -3.3540e-02,  1.8438e-01,\n","         1.3968e-02,  1.6821e-03, -5.5849e-02, -8.0941e-01,  3.8881e-01,\n","         3.4365e-01, -4.1117e-01, -2.0996e-01, -1.0344e-01, -4.0679e-01,\n","         2.8167e+00, -9.6546e-02, -2.9524e-01, -9.5187e-02, -2.2905e-01,\n","         2.4526e-01, -1.6460e-01, -5.5657e-01, -3.6732e-02, -2.2867e-01,\n","        -1.6067e-01,  1.3233e-01, -2.2837e-01,  2.5995e-01, -8.4858e-03,\n","        -1.0144e+00,  1.2996e-01, -1.2879e-01,  3.8767e-01,  1.8762e-01,\n","        -3.8737e-01,  3.1376e-01, -7.3456e-01,  4.6178e-01,  4.6601e-01,\n","        -5.7279e-02, -2.1166e-01, -1.9663e+00,  3.8102e-01,  2.2589e-01,\n","         3.0810e-01,  2.5124e-01,  1.6685e-01, -1.7924e-01, -2.7407e-01,\n","         4.5863e-02,  4.5874e-01, -1.2928e-01, -1.5197e-01, -4.1277e-01,\n","         3.1108e-01, -3.5875e-02,  2.4647e-01,  9.1209e-02,  1.3496e-01,\n","         2.0606e-02, -1.1449e-01, -2.7051e-02,  1.6689e-01,  3.3852e-03,\n","        -9.7864e-03,  1.1335e-01, -3.1098e-01,  1.7264e-01, -8.8281e-02,\n","        -2.2569e-01, -5.5169e-02, -2.6836e-01,  1.4090e-01,  1.7592e-01,\n","         4.9786e-03, -1.6281e-01,  2.6746e-01,  3.9190e-01, -1.0610e-01,\n","        -3.8932e-01, -1.3837e-01, -1.3374e-01,  6.3297e-02,  2.2207e-01,\n","        -3.7249e-01, -1.2220e-01,  2.8922e-01,  3.5506e-02, -2.0174e-01,\n","        -3.7285e-01,  2.4903e-01, -2.5731e-01, -4.3991e-01,  2.3476e-01,\n","         2.5616e-02,  3.2749e-01,  1.6465e-01,  5.0753e-02, -6.7244e-01,\n","         2.1953e-01,  4.1756e-02, -2.8575e-01, -4.6060e-01,  1.1920e-02,\n","         1.3773e-01, -1.2792e-01,  9.1088e-02, -2.2541e-01,  2.5843e-01,\n","        -1.4994e-01, -3.9694e-01,  4.7565e-01, -8.6202e-02,  9.6949e-03,\n","         5.5563e-01,  3.6184e-01,  2.4583e-01, -1.5536e-01,  2.8015e-01,\n","         1.1784e+00,  2.6259e-01, -4.8109e-01, -4.6087e-01, -4.1646e-01,\n","        -1.5751e-01,  3.6877e-02, -1.2398e+00, -3.4839e-01, -1.7673e-01,\n","         3.1261e-02, -3.2313e+00,  5.2254e-01,  3.0759e-01,  2.4908e-01,\n","         4.2149e-02,  1.4895e-01,  1.3280e-01, -5.6454e-01,  1.9543e-01,\n","        -2.3948e-01, -4.9030e-01, -3.9484e-01, -7.7505e-02,  3.0822e-01,\n","         2.3232e-01, -2.8774e-01,  1.8830e-01, -1.3518e-01, -3.8548e-01,\n","        -1.9003e-01,  3.1351e-01, -3.6871e-01,  6.1885e-01, -1.6753e-01,\n","         6.8572e-01,  4.0731e-01,  1.6437e-01,  2.6510e-01,  3.9992e+00,\n","         4.7597e-02,  9.1097e-02,  1.0259e-01, -2.5647e-01, -2.8696e-01,\n","        -8.6141e-02, -4.3966e-01, -2.6102e-01,  2.1829e-01,  1.0150e-01,\n","        -1.8444e-01, -1.2895e-01, -4.9873e-01, -1.0664e-01, -3.5579e-01,\n","        -2.3180e-01,  8.6545e-03,  2.1243e-01, -7.6511e-01, -1.2854e-01,\n","         4.3679e-01,  1.5846e-01, -1.3408e-01,  1.1934e-01, -3.6677e-01,\n","        -5.5662e-01, -5.4969e-01,  3.6808e-01,  2.4230e-02, -1.0814e+00,\n","         1.7460e-01,  1.7868e-01, -1.4528e-01,  8.4023e-02,  3.3936e-01,\n","        -3.1761e-01,  3.3891e-01, -1.7439e-01, -1.7383e-01, -1.4517e-02,\n","         9.4920e-03, -3.4670e-01, -2.2801e-01, -4.0658e-01,  9.4699e-02,\n","         4.1574e-02, -5.4721e-01,  1.6077e-01,  5.7045e-02, -1.5887e-02,\n","        -1.8827e-01, -1.5159e-01, -2.9561e-01,  1.7302e-01,  3.4877e-01,\n","         1.7287e-01,  3.9085e-02, -7.3917e-02, -1.1163e-01,  7.2685e-02,\n","         2.6188e-01,  1.4374e-01, -6.8238e-02,  4.1183e-01,  1.9411e-01,\n","         1.0892e-01, -3.2740e-01, -1.5114e-01, -3.2178e-02,  3.1989e-01,\n","        -1.9256e-01, -2.0294e+00,  2.6039e-01,  5.3210e-01, -3.4126e-01,\n","         1.0440e-01,  2.1031e-01,  1.0412e-01, -1.9871e-01,  1.9260e-01,\n","         4.6711e-01,  2.5916e-01,  3.8663e-01,  1.6859e-01, -4.9086e-02,\n","         1.1726e-01,  1.2388e-01,  6.6973e-01,  3.2925e-01, -4.2917e-01,\n","        -3.5519e-01, -7.8366e-02,  4.1667e-01, -1.6898e-02, -1.3027e-01,\n","         5.8862e-01, -1.2800e-01,  5.3281e-02,  5.1498e-03,  1.1792e-01,\n","         3.0008e-01,  1.7620e-01, -4.8659e-02, -2.2432e-01, -2.6372e-01,\n","        -2.1650e-01, -1.7877e-01,  3.3201e-01,  3.8241e-01, -1.5593e-01,\n","        -1.8369e-01,  1.4587e-01,  4.8484e-02, -2.2366e-01, -2.9394e-01,\n","         1.0220e-01, -5.1559e-02, -1.3901e-01, -1.5145e+00, -7.9092e-02,\n","        -4.8847e-02, -3.0673e-01, -2.2894e-01,  7.9123e-02, -2.0714e-01,\n","        -8.5067e-02,  1.4561e-01,  4.0261e-01,  3.2169e-01,  2.5772e-01,\n","         1.1420e-01, -5.3751e-02, -2.0642e-02, -3.5173e-01, -4.2789e-01,\n","         2.6998e-01,  2.6419e-03, -3.6448e-02,  4.2779e-01,  2.7270e-01,\n","        -5.1822e-01, -3.7839e-01,  2.2931e-02, -2.7526e-01, -3.4889e-01,\n","        -5.4904e-01,  2.9176e-01, -8.7513e-02, -1.2797e-01,  5.2686e+00,\n","        -7.2348e-01,  3.2521e-01,  1.5254e-01,  1.0246e-02,  6.7198e-02,\n","         1.6410e-01,  1.5005e-01,  5.7457e-01, -1.6311e-01, -4.0099e-01,\n","        -3.5496e-01,  2.2200e-01,  1.6090e-01, -3.2830e-01,  9.6469e-02,\n","        -3.1262e-01,  6.8539e-02,  9.7216e-02, -3.4155e-01, -4.0651e-01,\n","         5.5636e-02,  1.6953e-01, -2.2583e-01,  7.7536e-01,  5.3596e-01,\n","        -1.2859e-01, -5.2086e-01, -1.4417e-01, -3.5269e-01, -3.1792e-01,\n","         1.9880e-01,  1.5808e-02,  5.8677e-01,  3.2680e-01, -1.9595e-01,\n","         1.0380e-01,  1.0469e-01,  4.0632e-01, -8.3488e-02,  2.8314e-01,\n","         1.9067e-01,  9.7493e-01, -2.1578e-01,  4.0659e-01,  3.2359e-01,\n","        -5.2701e-02,  3.6178e-03,  4.5459e-02,  4.6516e-01,  6.7060e-02,\n","         2.1880e-01,  2.2476e-01, -5.6227e-01,  1.8627e-01, -9.3659e-02,\n","        -1.0859e-01, -4.3540e-02, -2.3267e-01,  2.0044e-01,  6.0795e-02,\n","         2.8609e-02,  3.3633e-03, -4.3769e-01, -4.9710e-02, -3.6383e-01,\n","         3.6268e-01, -2.9623e-01, -2.4136e-01, -6.0605e-01, -5.0104e-01,\n","        -9.5289e-02, -8.2130e-01, -4.3117e-01,  1.3454e-01,  2.6575e-01,\n","        -5.7185e-01,  3.4002e-02, -1.3778e-01, -4.4537e-03,  1.7165e-01,\n","         6.2144e-02,  1.1474e-01, -7.5025e-01,  6.9798e-02,  3.2467e-01,\n","        -6.9431e-01, -3.0386e-01,  4.4554e-01,  4.8391e-01, -2.5542e-01,\n","        -3.6392e-01,  2.7006e-02,  2.2393e-02,  1.8885e-01,  1.2165e-02,\n","        -2.9450e-01,  7.7751e-02, -2.9465e-01,  1.7627e-01,  5.1255e-01,\n","         9.3333e-02, -7.7583e-02,  2.3686e-01, -6.0814e-02, -8.5554e-02,\n","        -1.2325e-01,  1.3434e-01,  1.6336e-02, -2.4914e-01,  2.5172e-01,\n","         2.8847e-02,  8.0885e-03, -2.8119e-01,  2.3125e-01, -2.0573e-02,\n","        -1.9181e-01, -1.3482e-01, -6.8912e+00,  5.5886e-01,  1.5422e-01,\n","         9.5756e-02, -2.6526e-01, -1.1458e-01,  5.2413e-01,  4.1520e-01,\n","        -1.9809e-01,  1.0812e-01, -1.7872e-01, -2.8758e-01, -2.2845e+00,\n","         5.0803e-02, -2.1907e-01, -7.6228e-02,  2.0509e-01, -8.7298e-01,\n","        -8.5449e-02,  1.4827e-01, -2.9821e-01,  4.3970e-01, -1.1378e-01,\n","         5.5606e-01,  4.0387e-01,  2.0326e-01,  8.5840e-02, -3.7614e-01,\n","         2.4857e-01, -3.1706e-01,  1.1883e-02, -1.8205e-01,  4.7037e-01,\n","        -1.7306e-01, -1.8070e-01, -4.4879e-01,  1.1347e-01, -2.2446e-01,\n","         1.4078e-01, -7.3016e-01,  9.6628e-02, -3.4889e-01,  3.5453e-01,\n","         3.2981e-01, -7.4592e-01,  1.9603e-01, -6.4582e-01, -2.5627e+00,\n","         2.1793e-01, -4.2047e-02,  5.1063e-01,  3.1593e-01, -1.7099e-01,\n","        -2.6736e-01, -6.1461e-02,  1.1684e-01,  3.5648e-01, -1.9143e-01,\n","         4.0081e-01, -2.8117e-01, -4.0522e-01,  2.2744e-02,  3.6856e-01,\n","        -1.5640e-01, -3.2261e-01, -1.3640e-01, -3.8982e-01, -2.4817e-01,\n","        -2.3443e-01, -2.3304e-01, -2.8962e-01, -9.8832e-02, -2.9107e-01,\n","         6.2880e-03,  3.3252e-01,  4.1405e-01, -1.1586e-01, -1.1099e-01,\n","        -1.1027e-01,  1.6092e-01, -3.8889e-01, -7.4945e-02,  5.7417e-01,\n","         2.8122e-01,  1.1946e-01, -2.1556e-01, -4.1677e-01, -2.5173e-01,\n","         2.9711e-03, -1.3603e-01,  3.4512e-02, -6.2017e-02, -1.7346e-01,\n","        -2.4494e-02, -2.0006e-01,  3.9713e-01, -2.4897e-02, -4.7831e-02,\n","        -3.9623e-01, -2.7633e-01, -1.4263e-01,  2.8597e-01, -3.9340e-01,\n","        -2.3304e-01, -1.5023e-01,  2.2876e-01,  2.4928e+00,  3.5083e-01,\n","        -2.0310e-01, -3.2030e-01, -4.5908e-01,  5.6730e-01, -1.1396e-01,\n","         4.4320e-01,  1.7212e+00, -2.2990e-01, -4.4117e-02, -1.9487e-01,\n","         9.4585e-02,  3.2772e-01, -3.4377e-01,  5.5012e-01, -5.5770e-02,\n","        -6.3252e-02,  7.3237e-02,  2.6462e-01,  2.6804e-01, -3.8656e-01,\n","         5.1245e-01,  7.8772e-02, -2.5365e-01,  1.8276e-01,  1.8112e-01,\n","        -2.3906e-01,  3.7087e-02, -3.9700e-01,  6.5618e-02, -7.2795e-02,\n","        -3.1432e-02,  1.0966e+00, -1.0924e-01,  1.0694e-01,  4.1502e-01,\n","         3.5799e-01,  4.0251e-02, -2.1573e-01, -1.9640e-01,  1.0356e-01,\n","        -4.0683e-01,  5.6883e-01, -2.9615e-03, -4.3482e-02, -6.5816e-01,\n","        -3.8862e-01,  2.3494e-01,  6.2383e-03,  6.4747e-02, -3.5057e-01,\n","        -2.2235e-01,  1.7590e-01, -7.6272e-02, -3.1964e-02, -6.5666e-01,\n","         8.9846e-02, -5.1595e-03,  1.0866e-01, -2.1952e-01,  1.3246e-01,\n","        -2.8440e-01, -9.0164e-03, -1.5787e-01, -2.5948e-01,  4.6393e-01,\n","        -5.9641e-01, -1.7103e-01, -2.4834e-01,  3.0567e-01, -5.9610e-02,\n","        -2.1686e+00, -1.0826e-01, -2.5444e-01, -2.3829e-01, -3.9330e-01,\n","        -2.6673e-01,  5.6668e-01, -5.6012e-04, -1.4325e-01,  1.5828e-01,\n","         3.3744e-03,  1.8766e+00,  1.4102e-01, -2.3894e-02, -2.1943e-01,\n","        -5.8150e-02,  2.2983e-02, -1.9514e-01,  1.3331e-01, -4.4473e-01,\n","        -2.6641e-01,  1.4212e+00, -1.4298e-02, -3.4267e-02,  1.3462e-01,\n","         1.1318e-01,  1.2614e-01,  1.8576e-01, -1.2887e-01,  1.3839e-01,\n","         1.1759e-01,  5.5201e-01,  2.1543e-02], grad_fn=<SelectBackward0>)\n"]}],"source":["# Get the output for the [CLS] token. Call it cls_first_out.\n","### YOUR CODE HERE\n","\n","cls_first_out = bert_toy_outputs.last_hidden_state[0][0]\n","\n","### END YOUR CODE\n","print('First output of [CLS] token: ', cls_first_out)"]},{"cell_type":"markdown","metadata":{"id":"DMiyr5izQPaC"},"source":["**QUESTION:**\n","\n","3.b What is the first value of the output of the [CLS] token?"]},{"cell_type":"markdown","metadata":{"id":"ABg5ttdeWfo4"},"source":["Now we construct the dataset and the dataloader. The BERT dataset class is defined at the beginning."]},{"cell_type":"code","execution_count":38,"metadata":{"id":"9USxNoQ_WgIg"},"outputs":[{"name":"stderr","output_type":"stream","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (621 > 512). Running this sequence through the model will result in indexing errors\n"]}],"source":["bert_train_data = BERTClassificationData(imdb_train_set,\n","                                tokenizer=bert_tokenizer,\n","                                max_len=100,\n","                                num_examples=-1\n","                                )\n","\n","bert_test_data = BERTClassificationData(imdb_test_set,\n","                                tokenizer=bert_tokenizer,\n","                                max_len=100,\n","                                num_examples=-1\n","                                )\n","\n","batch_size = 4\n","bert_imdb_train_loader = DataLoader(bert_train_data, batch_size=batch_size, shuffle=True)\n","bert_imdb_test_loader = DataLoader(bert_test_data, batch_size=batch_size, shuffle=True)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"OeghP0kJTqpB"},"source":["Now build the classification network that uses the output of the [CLS] token for the classification."]},{"cell_type":"code","execution_count":39,"metadata":{"id":"Yr-vOerxuf0_"},"outputs":[],"source":["%%capture\n","\n","class BERTClassificationNetworkClass(torch.nn.Module):\n","    \"\"\"\n","    Write the class for the classification network using\n","    the Masked Language Model BERT.\n","    Specificaly, you will need to extract the output of the [CLS] token\n","    from the BERT model (i.e., the very first token), apply a suitable linear layer,\n","    and apply the sigmoid function.\n","    \"\"\"\n","\n","    def __init__(self):\n","        ### YOUR CODE HERE\n","\n","        super(BERTClassificationNetworkClass, self).__init__()\n","        self.bert_model = bert_model\n","        self.linear = torch.nn.Linear(768, 1)\n","        self.sigmoid = torch.nn.Sigmoid()\n","\n","        ### END YOUR CODE\n","\n","    def forward(self, x):\n","        # Get the forward pass. Apply the BERT model, then the linear layer, and\n","        # then apply the sigmoid\n","        ### YOUR CODE HERE\n","\n","        outputs = self.bert_model(**x)\n","        cls_output = outputs.last_hidden_state[:, 0, :]\n","        linear_output = self.linear(cls_output)\n","        sigmoid_output = self.sigmoid(linear_output)\n","\n","        ### END YOUR CODE\n","\n","        return torch.squeeze(sigmoid_output) # removing 'x 1 x ' dimensions\n","\n","\n","loss_fn = torch.nn.BCELoss()\n","\n","bert_classification_model = BERTClassificationNetworkClass().to(device)\n"]},{"cell_type":"markdown","metadata":{"id":"Jja3T50uYlde"},"source":["Let's test it. Is the structure correct?"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"_K1PVgiVYT23"},"outputs":[{"name":"stdout","output_type":"stream","text":["Output:  tensor([0.4195, 0.4163, 0.4187, 0.4307], grad_fn=<SqueezeBackward0>)\n","Loss:  tensor(0.6988, grad_fn=<BinaryCrossEntropyBackward0>)\n"]}],"source":["test = next(iter(bert_imdb_train_loader))\n","\n","out = bert_classification_model({'input_ids': test['input_ids']})\n","\n","loss = loss_fn(out.float(), test['label'].float())\n","\n","print('Output: ', out)\n","print('Loss: ', loss)\n"]},{"cell_type":"markdown","metadata":{"id":"iPcaxhduYzxK"},"source":["Good. Finally, we need train and test loops:"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"OydE2YjmYM6p"},"outputs":[],"source":["def bert_train_loop(dataloader, model, loss_fn, optimizer, reporting_interval=100, steps=None):\n","    \"\"\"\n","    Following the same logic as above, write the training loop to use the\n","    Masked Language Model BERT for the sentiment classification task. You\n","    only need to report the average loss after the reporting interval\n","    and end of each epoch.\n","    \"\"\"\n","\n","    ### YOUR CODE HERE\n","\n","    model.train()\n","    epoch_loss = 0\n","    batch_count = 0\n","\n","    for step, batch_data in enumerate(dataloader):\n","        if steps and step >= steps:\n","            break\n","    \n","        batch_input = {k: v.to(device) for k, v in batch_data.items()}\n","    \n","        optimizer.zero_grad()\n","    \n","        batch_output = model(batch_input)\n","        batch_loss = loss_fn(batch_output, batch_input['label'].float())\n","    \n","        batch_loss.backward()\n","        optimizer.step()\n","    \n","        epoch_loss += batch_loss.item()\n","        batch_count += 1\n","    \n","        if step % reporting_interval == 0 and step > 0:\n","            avg_loss = epoch_loss / batch_count\n","            print(f\"Step {step}, Avg train loss: {avg_loss:.6f}\")\n","\n","    if batch_count == 0:\n","        print(\"No batches were processed.\")\n","        return\n","    \n","    avg_epoch_loss = epoch_loss / batch_count\n","\n","    ### END YOUR CODE\n","\n","    print(f\"Training Results: \\n  Avg train loss: {epoch_loss/batch:>8f} \\n\")\n","\n","\n","def bert_test_loop(dataloader, model, loss_fn, reporting_interval=100, contrast_pair=None,steps=None):\n","    \"\"\"\n","    Following the same logic as above, write the test loop to use the\n","    Masked Language Model BERT for the sentiment classification task.\n","    Please report on the accuracy after the reporting interval and end of each epoch.\n","    \"\"\"\n","\n","    ### YOUR CODE HERE\n","\n","    model.eval()\n","    test_loss, correct_class_predictions\n","\n","    for step, batch_data in enumerate(dataloader):\n","        if steps and step >= steps:\n","            break\n","\n","        batch_input = {k: v.to(device) for k, v in batch_data.items()}\n","\n","        batch_output = model(batch_input)\n","        batch_loss = loss_fn(batch_output, batch_input['label'].float())\n","        test_loss += batch_loss.item()\n","\n","        for i in range(batch_output.size(0)):\n","            predicted_class = torch.round(batch_output[i]).item()\n","            correct_class = batch_input['label'][i].item()\n","\n","            if predicted_class == correct_class:\n","                correct_class_predictions += 1\n","\n","    avg_test_loss = test_loss / len(dataloader.dataset)\n","\n","    ### END YOUR CODE\n","\n","    correct = float(correct.cpu().detach().numpy())\n","\n","    test_loss /= batch\n","    correct /= total\n","    print(correct)\n","\n","    print(f\"Test Results: \\n Test Accuracy: {(100*correct):>0.1f}%, Avg test loss: {test_loss:>8f} \\n\")"]},{"cell_type":"markdown","metadata":{"id":"x7L9eebiuf0_"},"source":["Now let's see:"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"weVynJqHuf0_"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1\n","-------------------------------\n"]},{"ename":"TypeError","evalue":"forward() got an unexpected keyword argument 'label'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[42], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-------------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mbert_train_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbert_imdb_train_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbert_classification_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madam_optimizer_bert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     bert_test_loop(bert_imdb_test_loader, bert_classification_model, loss_fn,\n\u001b[1;32m      8\u001b[0m               steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m\n\u001b[1;32m      9\u001b[0m               ) \u001b[38;5;66;03m# no optimizer use here!\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","Cell \u001b[0;32mIn[41], line 23\u001b[0m, in \u001b[0;36mbert_train_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer, reporting_interval, steps)\u001b[0m\n\u001b[1;32m     19\u001b[0m batch_input \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch_data\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     21\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 23\u001b[0m batch_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m batch_loss \u001b[38;5;241m=\u001b[39m loss_fn(batch_output, batch_input[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m     26\u001b[0m batch_loss\u001b[38;5;241m.\u001b[39mbackward()\n","File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[39], line 25\u001b[0m, in \u001b[0;36mBERTClassificationNetworkClass.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Get the forward pass. Apply the BERT model, then the linear layer, and\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# then apply the sigmoid\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m### YOUR CODE HERE\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     cls_output \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\n\u001b[1;32m     27\u001b[0m     linear_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(cls_output)\n","File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","\u001b[0;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'label'"]}],"source":["adam_optimizer_bert = torch.optim.AdamW(bert_classification_model.parameters(), lr=0.00001)\n","\n","epochs = 1\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    bert_train_loop(bert_imdb_train_loader, bert_classification_model, loss_fn, adam_optimizer_bert, steps=2000)\n","    bert_test_loop(bert_imdb_test_loader, bert_classification_model, loss_fn,\n","              steps=500\n","              ) # no optimizer use here!\n","print(\"Done!\")"]},{"cell_type":"markdown","metadata":{"id":"_U8i12wAuf1A"},"source":["\n","**QUESTION:**\n","\n","3.c What was the test accuracy you got for the BERT model?\n","\n","\n","And that is it. Congratulations!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m2IadZGJM5jU"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.15"}},"nbformat":4,"nbformat_minor":0}
